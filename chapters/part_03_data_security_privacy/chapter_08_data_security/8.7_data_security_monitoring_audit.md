# 8.7 数据安全监控与审计

## 概述

数据安全监控与审计通过对数据访问、操作、传输的全生命周期实时监控和事后审计，实现安全威胁的早期发现和合规证据的完整留存。监控体系需要回答三个核心问题：谁在什么时间通过什么方式访问或修改了哪些数据、这些行为是否偏离正常基线，以及如何为事件响应和合规审查提供可验证的证据链。

本节覆盖数据库活动监控 (DAM)、用户行为分析 (UEBA)、数据外泄检测、审计日志管理及取证分析技术，构建可观测、可验证、可响应的数据安全监控体系。

→ *详见 [11.6 安全监控](../../part_04_security_operations_defense_capabilities/chapter_11_security_operations/11.6-security-monitoring.md) 中关于 SOC 与数据安全监控联动、UEBA 行为基线构建的完整运营指南*

---

## 适用边界

**适用场景**：

- 需满足合规要求 (SOX、GDPR、PIPL、PCI-DSS) 的审计证据留存
- 检测内部威胁（特权用户滥用、离职员工数据窃取）
- 高价值数据资产保护（财务数据、客户 PII、知识产权）
- 满足监管机构调查取证需求（72 小时数据泄露通知）

**不适用场景**：

- 非结构化数据（邮件附件、文档内容）的实时监控需依赖 DLP 系统补充
- 加密数据库（如端到端加密）无法在不解密情况下进行内容级监控
- 极高吞吐量场景（每秒 >10 万次查询）可能需要采样监控而非全量

---

## 关键约束

1. **性能开销**：DAM 代理模式会增加 5-15% 数据库延迟，网络代理模式可能成为单点瓶颈，需在监控覆盖度与性能间权衡
2. **存储成本**：全量审计日志存储成本显著（内部测算：生产数据库审计日志约为业务数据的 10-20%），需定义保留策略与归档机制
3. **误报率**：UEBA 基线学习期（通常 30-90 天）内误报率可能达 20-30%，需设计白名单与阈值调优流程
4. **加密流量**：TLS 加密的数据库连接需在 DAM 层解密后重新加密，涉及密钥管理复杂度
5. **合规窗口**：GDPR 要求 72 小时内通知数据泄露，审计日志检索与分析能力需满足该时效要求

---

## 常见误区

1. **误区一：仅记录高风险操作**

   - 问题：审计日志缺失导致取证时无法重建完整攻击链（如攻击者先进行低风险侦察查询，再实施高风险数据导出）
   - 正确做法：记录所有数据访问，但可对低风险操作采用更短保留期或压缩存储
2. **误区二：审计日志与业务数据共存**

   - 问题：攻击者获得数据库权限后可篡改或删除审计日志，导致证据链断裂
   - 正确做法：审计日志写入独立 WORM 存储（如 AWS S3 Object Lock、Azure Immutable Blob）或专用审计服务器
3. **误区三：UEBA 基线一次建立，长期复用**

   - 问题：用户职责变更（如晋升、岗位调整）后行为模式改变，导致大量误报或漏报
   - 正确做法：定期重建基线（建议每季度），并在用户角色变更后触发增量更新
4. **误区四：将 DAM 告警直接发送给 DBA**

   - 问题：DBA 本身可能是内部威胁主体，或告警疲劳导致关键事件被忽略
   - 正确做法：高风险告警路由至独立安全团队 (SOC)，DBA 仅接收运维类告警

---

## 验证方法

1. **DAM 策略有效性验证**：

   - 构造测试用例（如执行 DROP TABLE，全表无 WHERE 查询，批量数据导出），验证策略能否触发告警并记录完整审计日志
   - 红队测试：模拟内部威胁场景（如夜间异地登录 + 敏感表全量导出），检查多层检测是否触发
2. **UEBA 基线准确性验证**：

   - 选取已知正常用户活动，验证误报率是否 <15%（行业可接受阈值）
   - 注入已知异常行为（如 Impossible Travel 场景：用户 1 小时内从北京和伦敦登录），验证检出率
3. **审计日志完整性验证**：

   - 定期执行链完整性校验（验证哈希链连续性与序列号无跳跃）
   - 模拟审计日志篡改（修改历史记录），验证完整性校验能否检测
4. **取证能力验证**：

   - 模拟数据泄露场景，要求在 2 小时内还原"用户 X 在时间 T 访问表 Y 并导出 Z 行数据"的完整证据链
   - 验证审计日志检索性能（P95 响应时间 <5 秒）

---

## 运行指标

| 指标类别           | 关键指标            | 目标阈值  | 触发条件                                              |
| ------------------ | ------------------- | --------- | ----------------------------------------------------- |
| **检测能力** | 威胁检测覆盖率      | >95%      | 每季度评估，覆盖 MITRE ATT&CK 数据访问相关 TTP        |
|                    | 平均检测时间 (MTTD) | <10 分钟  | 从异常行为发生到告警生成的时长                        |
|                    | 误报率              | <15%      | 告警后经人工确认为误报的比例                          |
| **响应效率** | 平均响应时间 (MTTR) | <30 分钟  | 从告警触发到初步遏制措施的时长                        |
|                    | 自动化处理率        | >70%      | 可通过 SOAR 自动处理的告警占比                        |
| **审计合规** | 日志完整性          | 100%      | 每日执行链完整性校验，任何失败立即告警                |
|                    | 日志保留达标率      | 100%      | 符合法规要求的保留期限（SOX：7 年，GDPR：依处理目的） |
|                    | 取证响应时效        | <2 小时   | 接到取证请求到提供完整证据链的时长                    |
| **系统健康** | 监控系统可用性      | >99.9%    | DAM/UEBA 系统月度可用性                               |
|                    | 日志接收延迟        | <30 秒    | 从数据库操作发生到审计日志写入的延迟                  |
|                    | 存储告警阈值        | 容量 <80% | 审计日志存储空间使用率                                |

**告警升级机制**：

- CRITICAL 级别告警（如权限提升、大规模数据导出）：立即通知 SOC 团队与 CISO
- HIGH 级别告警（如首次访问敏感表）：通知 SOC 团队，记录到事件队列
- MEDIUM 级别告警（如非工作时间访问）：记录到审计日志，每日汇总报告
- LOW 级别告警（如查询复杂度偏高）：仅记录，用于基线调优

---

## 8.7.1 数据库活动监控 (DAM)

### DAM 架构与部署模式

数据库活动监控 (Database Activity Monitoring, DAM) 通过实时捕获和分析数据库操作，提供细粒度的访问审计、权限滥用检测和异常查询告警。DAM 的核心能力在于：在不修改应用代码的前提下，获取完整的数据访问上下文（用户身份、来源 IP、SQL 语句、返回行数、执行时间），并基于策略引擎进行实时风险评估。

![数据库安全架构](../../../assets/images/chapter_08/05_Database_Security_Architecture_v6.png)

上图展示了 DAM 在数据库安全架构中的位置：作为数据访问层与安全监控层之间的桥梁，DAM 需要捕获所有数据库操作（包括应用访问、DBA 直连、第三方工具），并将审计数据送入 SIEM/UEBA 进行关联分析。

```
┌─────────────────────────────────────────────────────────────┐
│ DAM管理控制台 │
│ (策略配置 | 告警管理 | 合规报告 | 风险仪表板) │
└──────────────────────┬──────────────────────────────────────┘
 │
 ┌─────────────┼─────────────┬────────────────────┐
 │ │ │ │
 ┌────▼────┐ ┌───▼────┐ ┌───▼─────┐ ┌─────────▼──────┐
 │ Network │ │ Agent │ │ Log │ │ Sniffer │
 │ Proxy │ │ (DBMS) │ │Collector│ │ (SPAN/TAP) │
 └─────────┘ └────────┘ └─────────┘ └────────────────┘
 │ │ │ │
 ┌────▼─────────────▼─────────────▼────────────────▼────────┐
 │ SQL解析与会话重组引擎 │
 │ (协议解码 | SQL语法分析 | 参数提取 | 用户映射) │
 └────────────────────────┬───────────────────────────────────┘
 │
 ┌────────────────────────▼───────────────────────────────────┐
 │ 行为分析与威胁检测引擎 │
 │ 规则引擎 | ML异常检测 | 权限滥用 | 数据外泄检测 │
 └────────────────────────┬───────────────────────────────────┘
 │
 ┌────────────────────────▼───────────────────────────────────┐
 │ 审计存储与检索(WORM存储 | 加密 | 索引) │
 └────────────────────────────────────────────────────────────┘
```

**部署模式对比**:

| 模式                       | 工作原理                | 优点                     | 缺点                 | 适用场景       |
| -------------------------- | ----------------------- | ------------------------ | -------------------- | -------------- |
| **网络代理 (Proxy)** | 数据库流量经过 DAM 代理 | 无需修改数据库、完整捕获 | 单点故障、延迟增加   | 高安全要求环境 |
| **Agent 模式**       | 在数据库主机部署代理    | 低延迟、上下文丰富       | 需安装软件、资源占用 | 生产环境推荐   |
| **日志收集**         | 解析数据库审计日志      | 无性能影响、易实施       | 延迟高、信息有限     | 合规审计为主   |
| **网络嗅探 (TAP)**   | 镜像流量到 DAM 传感器   | 零故障影响               | 加密流量需解密       | 监控为主场景   |

**部署决策树**：

1. 业务系统是否容忍 5-15% 延迟？
   - 否 → 选择 Agent 模式或网络嗅探
   - 是 → 可考虑网络代理模式（便于集中管理）
2. 数据库是否支持详细审计日志？
   - 是 → 可仅用日志收集模式（成本最低）
   - 否 → 必须使用主动捕获模式 (Proxy/Agent/Sniffer)
3. 是否有独立审计数据库权限？
   - 否 → 必须使用网络侧监控 (Proxy/Sniffer)，防止 DBA 篡改
   - 是 → 可使用 Agent 模式

### DAM 策略配置

DAM 策略定义了哪些行为需要告警或阻断。策略设计需平衡安全性与可用性：过于严格导致业务阻塞，过于宽松则漏报威胁。

```python
# DAM策略配置示例
dam_policies = {
    "high_risk_queries": {
        "name": "高风险SQL操作检测",
        "enabled": True,
        "conditions": [
            {
                "type": "sql_pattern",
                "patterns": [
                    r"DROP\s+(TABLE|DATABASE|SCHEMA)",
                    r"TRUNCATE\s+TABLE",
                    r"DELETE\s+FROM\s+\w+\s+WHERE\s+1\s*=\s*1",  # 全表删除
                    r"UPDATE\s+\w+\s+SET\s+.*\s+WHERE\s+1\s*=\s*1",  # 全表更新
                    r"GRANT\s+ALL\s+PRIVILEGES",
                    r"CREATE\s+USER",
                    r"ALTER\s+USER.*PASSWORD"
                ],
                "case_insensitive": True
            }
        ],
        "actions": [
            {"type": "alert", "severity": "critical", "recipients": ["dba-team@company.com"]},
            {"type": "block", "enabled": False},  # 可选阻断
            {"type": "record_query_result", "enabled": True}
        ],
        "exceptions": {
            "users": ["backup_service", "migration_script"],
            "time_windows": ["maintenance_window"]
        }
    },

    "mass_data_export": {
        "name": "大规模数据导出检测",
        "enabled": True,
        "conditions": [
            {
                "type": "result_set_size",
                "threshold": 10000,  # 返回超过1万行
                "measurement_window": "1 query"
            },
            {
                "type": "query_frequency",
                "threshold": 100,  # 1分钟内100次查询
                "measurement_window": "1 minute"
            }
        ],
        "data_classification_filter": ["CONFIDENTIAL", "RESTRICTED"],
        "actions": [
            {"type": "alert", "severity": "high"},
            {"type": "require_mfa", "enabled": True},
            {"type": "throttle", "max_rows_per_minute": 5000}
        ]
    },

    "privilege_escalation": {
        "name": "权限提升检测",
        "enabled": True,
        "conditions": [
            {
                "type": "permission_change",
                "operations": ["GRANT", "REVOKE", "ALTER USER", "CREATE ROLE"],
                "target_privileges": ["SUPER", "ALL PRIVILEGES", "WITH ADMIN OPTION"]
            }
        ],
        "actions": [
            {"type": "alert", "severity": "critical"},
            {"type": "require_dual_approval", "approvers": ["dba_lead", "security_officer"]},
            {"type": "auto_rollback", "delay_seconds": 300}  # 5分钟后自动回滚
        ]
    },

    "after_hours_access": {
        "name": "非工作时间访问",
        "enabled": True,
        "conditions": [
            {
                "type": "time_based",
                "business_hours": {
                    "timezone": "Asia/Shanghai",
                    "weekdays": "09:00-18:00",
                    "weekends": False
                },
                "outside_hours": True
            },
            {
                "type": "sensitive_tables",
                "tables": ["customers", "transactions", "employees.salary"]
            }
        ],
        "user_filter": {
            "exclude_roles": ["on_call_engineer", "global_support"]
        },
        "actions": [
            {"type": "alert", "severity": "medium"},
            {"type": "step_up_authentication", "mfa_required": True},
            {"type": "enhanced_logging", "log_query_results": True}
        ]
    },

    "sql_injection_detection": {
        "name": "SQL注入检测",
        "enabled": True,
        "conditions": [
            {
                "type": "sql_anomaly",
                "patterns": [
                    r"'\s*OR\s+'1'\s*=\s*'1",
                    r";\s*DROP\s+TABLE",
                    r"UNION\s+SELECT.*FROM\s+information_schema",
                    r"'.*--",
                    r"exec\s*\(\s*@",
                    r"xp_cmdshell"
                ]
            },
            {
                "type": "unexpected_source",
                "legitimate_app_servers": ["10.0.1.0/24", "10.0.2.0/24"],
                "from_outside_network": True
            }
        ],
        "actions": [
            {"type": "block", "enabled": True},
            {"type": "alert", "severity": "critical"},
            {"type": "isolate_connection", "duration_minutes": 30}
        ]
    }
}
```

**策略调优流程**：

1. 初始部署：启用所有策略的告警模式（不阻断），观察 1-2 周建立基线
2. 误报分析：统计误报率最高的策略，通过白名单（如合法批处理作业）或阈值调整降低误报
3. 渐进式阻断：对高置信度策略（如 SQL 注入检测）启用阻断，对业务影响大的策略（如大规模导出）仅告警
4. 持续优化：每月审查告警分布，识别新威胁模式并更新策略

### DAM 实现示例

以下代码展示 DAM 核心逻辑的简化实现，实际生产环境应使用成熟商业产品（如 Imperva、IBM Guardium）或开源方案（如 Percona Audit Plugin）。

```python
import re
from datetime import datetime, time
import sqlparse
from sqlparse.sql import IdentifierList, Identifier, Where
from sqlparse.tokens import Keyword, DML

class DatabaseActivityMonitor:
    def __init__(self, policy_engine):
        self.policy_engine = policy_engine
        self.session_tracker = {}
        self.baseline_models = {}

    def monitor_query(self, session_info, sql_query):
        """
        监控单个SQL查询
        """
        # 1. SQL解析
        parsed_query = self.parse_sql(sql_query)

        # 2. 提取元数据
        metadata = {
            "user": session_info["username"],
            "source_ip": session_info["client_ip"],
            "database": session_info["database"],
            "timestamp": datetime.now(),
            "sql_operation": parsed_query["operation"],
            "tables_accessed": parsed_query["tables"],
            "columns_accessed": parsed_query["columns"],
            "query_complexity": self.calculate_complexity(parsed_query),
            "estimated_result_rows": None  # 需数据库引擎支持
        }

        # 3. 策略评估
        violations = self.policy_engine.evaluate(metadata, sql_query)

        # 4. 异常检测
        anomalies = self.detect_anomalies(metadata)

        # 5. 执行动作
        for violation in violations:
            self.handle_violation(violation, metadata, sql_query)

        # 6. 记录审计日志
        self.log_audit_event(metadata, violations, anomalies, sql_query)

        # 7. 返回决策
        if any(v["action"] == "block" for v in violations):
            return {
                "allow": False,
                "reason": violations[0]["policy_name"],
                "user_message": "您的查询违反了数据安全策略，已被阻止。"
            }

        return {"allow": True}

    def parse_sql(self, sql_query):
        """
        解析 SQL 语句，提取操作类型和对象
        """
        parsed = sqlparse.parse(sql_query)[0]

        # 提取操作类型
        operation = None
        for token in parsed.tokens:
            if token.ttype is DML:
                operation = token.value.upper()
                break

        # 提取表名
        tables = []
        for token in parsed.tokens:
            if isinstance(token, IdentifierList):
                for identifier in token.get_identifiers():
                    tables.append(identifier.get_real_name())
            elif isinstance(token, Identifier):
                tables.append(token.get_real_name())

        # 提取列名（简化版）
        columns = self.extract_columns(parsed)

        return {
            "operation": operation,
            "tables": tables,
            "columns": columns,
            "has_where_clause": any(isinstance(t, Where) for t in parsed.tokens)
        }

    def detect_anomalies(self, metadata):
        """
        基于历史基线检测异常行为
        """
        user = metadata["user"]
        anomalies = []

        # 获取用户基线
        baseline = self.baseline_models.get(user, {})

        # 异常 1：访问从未访问过的表
        if metadata["tables_accessed"]:
            historical_tables = baseline.get("accessed_tables", set())
            new_tables = set(metadata["tables_accessed"]) - historical_tables

            if new_tables:
                anomalies.append({
                    "type": "first_time_table_access",
                    "tables": list(new_tables),
                    "risk_score": 40
                })

        # 异常 2：查询复杂度异常
        avg_complexity = baseline.get("avg_query_complexity", 5)
        if metadata["query_complexity"] > avg_complexity * 3:
            anomalies.append({
                "type": "high_complexity_query",
                "complexity": metadata["query_complexity"],
                "baseline": avg_complexity,
                "risk_score": 30
            })

        # 异常 3：非常规时间访问
        if not self.is_business_hours(metadata["timestamp"]):
            if baseline.get("after_hours_access_count", 0) < 5:  # 历史上很少非工作时间访问
                anomalies.append({
                    "type": "unusual_time_access",
                    "risk_score": 25
                })

        # 异常 4：高频查询
        recent_queries = self.get_recent_query_count(user, minutes=5)
        if recent_queries > baseline.get("avg_queries_per_5min", 10) * 5:
            anomalies.append({
                "type": "query_burst",
                "current_rate": recent_queries,
                "baseline": baseline.get("avg_queries_per_5min", 10),
                "risk_score": 35
            })

        return anomalies

    def calculate_complexity(self, parsed_query):
        """
        计算 SQL 复杂度（简化版）
        """
        score = 0
        score += len(parsed_query["tables"]) * 2  # 表数量
        score += len(parsed_query["columns"]) * 1  # 列数量
        score += 10 if not parsed_query["has_where_clause"] else 0  # 无 WHERE 子句
        # 可扩展：JOIN 数量、子查询层级、聚合函数等
        return score

    def is_business_hours(self, timestamp):
        """
        判断是否为工作时间
        """
        business_start = time(9, 0)
        business_end = time(18, 0)
        is_weekday = timestamp.weekday() < 5  # 0=周一，6=周日

        return is_weekday and business_start <= timestamp.time() <= business_end

    def log_audit_event(self, metadata, violations, anomalies, sql_query):
        """
        记录审计事件（生产环境应写入不可变存储）
        """
        import hashlib

        audit_record = {
            "timestamp": metadata["timestamp"].isoformat(),
            "event_id": self.generate_event_id(),
            "user": metadata["user"],
            "source_ip": metadata["source_ip"],
            "database": metadata["database"],
            "sql_operation": metadata["sql_operation"],
            "tables": metadata["tables_accessed"],
            "query_hash": hashlib.sha256(sql_query.encode()).hexdigest()[:16],
            "full_query": sql_query if any(v["action"] in ["alert", "block"] for v in violations) else None,
            "violations": [v["policy_name"] for v in violations],
            "anomalies": [a["type"] for a in anomalies],
            "risk_score": sum(a["risk_score"] for a in anomalies),
            "action_taken": "blocked" if any(v["action"] == "block" for v in violations) else "allowed"
        }

        # 写入审计日志（示例：写入文件，生产应使用专用审计系统）
        import json
        with open("/var/log/dam/audit.jsonl", "a") as f:
            f.write(json.dumps(audit_record) + "\n")

        # 同时发送到 SIEM
        # self.siem_forwarder.send(audit_record)

# 使用示例
from policy_engine import PolicyEngine

policy_engine = PolicyEngine(policies=dam_policies)
dam = DatabaseActivityMonitor(policy_engine)

# 监控查询
session = {
    "username": "john.doe",
    "client_ip": "10.0.5.123",
    "database": "production_db"
}

query = "SELECT * FROM customers WHERE email LIKE '%@example.com' LIMIT 50000"
result = dam.monitor_query(session, query)

if not result["allow"]:
    print(f"Query blocked: {result['reason']}")
else:
    print("Query allowed")
```

---

## 8.7.2 用户行为分析 (UEBA)

用户和实体行为分析 (User and Entity Behavior Analytics, UEBA) 通过机器学习建立正常行为基线，识别偏离基线的异常活动。UEBA 相比传统规则引擎的优势在于：能够检测未知威胁（如 0-day 攻击）和缓慢渐进式攻击（如攻击者用数周时间逐步扩大访问范围）。

### UEBA 架构与工作流程

```
┌───────────────────────────────────────────────────────────────┐
│ 数据源(多维度) │
│ DAM日志 | VPN日志 | 文件访问 | 邮件活动 | 云API调用 | DLP事件│
└────────────────────────┬──────────────────────────────────────┘
 │
┌────────────────────────▼──────────────────────────────────────┐
│ 数据归一化与实体解析 │
│ (用户身份统一 | 资源标准化 | 时间对齐 | 上下文关联) │
└────────────────────────┬──────────────────────────────────────┘
 │
┌────────────────────────▼──────────────────────────────────────┐
│ 基线建模(ML) │
│ 正常行为画像 | 同侪组分析 | 时序模式学习 │
└────────────────────────┬──────────────────────────────────────┘
 │
┌────────────────────────▼──────────────────────────────────────┐
│ 异常检测引擎 │
│ 统计异常 | 规则引擎 | 深度学习模型 | 威胁情报关联 │
└────────────────────────┬──────────────────────────────────────┘
 │
┌────────────────────────▼──────────────────────────────────────┐
│ 风险评分与告警 │
│ 多维度风险聚合 | 优先级排序 | 自动化响应 │
└───────────────────────────────────────────────────────────────┘
```

**UEBA 关键能力**：

1. **同侪组分析** (Peer Group Analysis)：将用户按部门/角色分组，检测个体行为是否偏离群体
2. **时序异常检测** (Temporal Anomaly)：识别行为模式的时间偏离（如平时 9-18 点工作的用户突然凌晨 3 点登录）
3. **实体关联** (Entity Linking)：关联用户跨系统行为（如同一用户在数据库、文件服务器、邮件系统的操作）
4. **威胁情报集成** (Threat Intelligence)：将用户行为与已知攻击模式 (MITRE ATT&CK) 关联

### UEBA 异常检测模型

```python
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import pandas as pd
from datetime import datetime, timedelta

class UEBAEngine:
    def __init__(self):
        self.user_baselines = {}
        self.peer_groups = {}
        self.anomaly_detector = IsolationForest(contamination=0.05, random_state=42)
        self.scaler = StandardScaler()

    def build_user_baseline(self, user_id, historical_days=90):
        """
        为用户建立行为基线
        """
        # 1. 收集历史数据
        end_date = datetime.now()
        start_date = end_date - timedelta(days=historical_days)

        user_events = self.fetch_user_events(user_id, start_date, end_date)

        # 2. 特征工程
        features = self.extract_behavior_features(user_events)

        # 3. 统计基线
        baseline = {
            "user_id": user_id,
            "observation_period": f"{start_date.date()} to {end_date.date()}",

            # 数据访问模式
            "avg_daily_queries": features["daily_queries"].mean(),
            "std_daily_queries": features["daily_queries"].std(),
            "avg_query_result_rows": features["result_rows"].mean(),
            "max_query_result_rows": features["result_rows"].quantile(0.95),  # 95 分位

            # 访问资源
            "accessed_databases": set(features["databases"]),
            "accessed_tables": set(features["tables"]),
            "primary_work_hours": self.identify_work_hours(features["timestamps"]),

            # 网络行为
            "login_locations": self.extract_locations(features["source_ips"]),
            "avg_session_duration_minutes": features["session_durations"].mean(),

            # 文件操作
            "avg_daily_file_downloads": features["daily_downloads"].mean(),
            "avg_file_size_mb": features["file_sizes"].mean(),

            # 同侪组
            "peer_group": self.assign_peer_group(user_id, features),

            "last_updated": datetime.now()
        }

        self.user_baselines[user_id] = baseline
        return baseline

    def extract_behavior_features(self, events):
        """
        从原始事件提取行为特征
        """
        df = pd.DataFrame(events)

        features = {
            "daily_queries": df.groupby(df["timestamp"].dt.date).size(),
            "result_rows": df["result_rows"],
            "databases": df["database"].unique(),
            "tables": df["table"].unique(),
            "timestamps": df["timestamp"],
            "source_ips": df["source_ip"].unique(),
            "session_durations": df.groupby("session_id")["duration"].sum(),
            "daily_downloads": df[df["action"] == "download"].groupby(df["timestamp"].dt.date).size(),
            "file_sizes": df[df["action"] == "download"]["file_size_mb"]
        }

        return features

    def detect_anomalies(self, user_id, current_activity):
        """
        检测当前活动的异常
        """
        baseline = self.user_baselines.get(user_id)
        if not baseline:
            return {"error": "No baseline available for user"}

        anomalies = []
        risk_score = 0

        # 异常 1：查询量激增
        current_queries = current_activity["query_count"]
        if current_queries > baseline["avg_daily_queries"] + 3 * baseline["std_daily_queries"]:
            anomalies.append({
                "type": "query_volume_spike",
                "severity": "high",
                "current_value": current_queries,
                "baseline_avg": baseline["avg_daily_queries"],
                "deviation_sigma": (current_queries - baseline["avg_daily_queries"]) / max(baseline["std_daily_queries"], 1)
            })
            risk_score += 40

        # 异常 2：访问从未访问的数据库/表
        new_databases = set(current_activity["databases"]) - baseline["accessed_databases"]
        new_tables = set(current_activity["tables"]) - baseline["accessed_tables"]

        if new_databases or new_tables:
            anomalies.append({
                "type": "first_time_resource_access",
                "severity": "medium",
                "new_databases": list(new_databases),
                "new_tables": list(new_tables)
            })
            risk_score += 30

        # 异常 3：异常登录地点
        current_location = self.geolocate_ip(current_activity["source_ip"])
        if current_location["country"] not in [loc["country"] for loc in baseline["login_locations"]]:
            anomalies.append({
                "type": "impossible_travel",
                "severity": "critical",
                "current_location": current_location,
                "baseline_locations": baseline["login_locations"]
            })
            risk_score += 60

        # 异常 4：非常规工作时间
        current_hour = current_activity["timestamp"].hour
        work_hours = baseline["primary_work_hours"]
        if current_hour < work_hours["start"] or current_hour > work_hours["end"]:
            anomalies.append({
                "type": "off_hours_activity",
                "severity": "low",
                "current_hour": current_hour,
                "typical_hours": f"{work_hours['start']}:00-{work_hours['end']}:00"
            })
            risk_score += 15

        # 异常 5：大规模数据导出
        if current_activity.get("total_downloaded_rows", 0) > baseline["max_query_result_rows"] * 2:
            anomalies.append({
                "type": "mass_data_exfiltration",
                "severity": "critical",
                "current_rows": current_activity["total_downloaded_rows"],
                "baseline_max": baseline["max_query_result_rows"]
            })
            risk_score += 70

        # 异常 6：同侪组偏离（与同职能人员行为对比）
        peer_deviation = self.compare_with_peers(user_id, current_activity)
        if peer_deviation["anomaly_score"] > 0.8:
            anomalies.append({
                "type": "peer_group_deviation",
                "severity": "medium",
                "details": peer_deviation
            })
            risk_score += 25

        # 综合风险评分
        return {
            "user_id": user_id,
            "timestamp": current_activity["timestamp"],
            "risk_score": min(risk_score, 100),
            "risk_level": self.classify_risk(risk_score),
            "anomalies": anomalies,
            "recommended_actions": self.recommend_actions(risk_score, anomalies)
        }

    def compare_with_peers(self, user_id, current_activity):
        """
        与同侪组对比
        """
        baseline = self.user_baselines[user_id]
        peer_group_id = baseline["peer_group"]

        # 获取同侪组所有用户的基线
        peer_baselines = [
            bl for uid, bl in self.user_baselines.items()
            if bl["peer_group"] == peer_group_id and uid != user_id
        ]

        if not peer_baselines:
            return {"anomaly_score": 0}

        # 对比关键指标
        peer_avg_queries = np.mean([pb["avg_daily_queries"] for pb in peer_baselines])
        peer_std_queries = np.std([pb["avg_daily_queries"] for pb in peer_baselines])

        current_queries = current_activity["query_count"]
        z_score = (current_queries - peer_avg_queries) / max(peer_std_queries, 1)

        return {
            "anomaly_score": min(abs(z_score) / 3, 1.0),  # 归一化到 0-1
            "peer_group": peer_group_id,
            "current_value": current_queries,
            "peer_average": peer_avg_queries,
            "z_score": z_score
        }

    def classify_risk(self, risk_score):
        """
        风险等级分类
        """
        if risk_score >= 70:
            return "CRITICAL"
        elif risk_score >= 50:
            return "HIGH"
        elif risk_score >= 30:
            return "MEDIUM"
        else:
            return "LOW"

    def recommend_actions(self, risk_score, anomalies):
        """
        基于风险评分推荐响应动作
        """
        actions = []

        if risk_score >= 70:
            actions.extend([
                "立即通知 SOC 团队",
                "要求用户进行 MFA 再认证",
                "临时冻结账号（pending 调查）",
                "启动事件响应流程",
                "保留取证数据（内存/网络/日志）"
            ])
        elif risk_score >= 50:
            actions.extend([
                "通知用户直属经理",
                "增强监控（记录所有操作详情）",
                "限制敏感数据访问",
                "人工审查活动日志"
            ])
        elif risk_score >= 30:
            actions.extend([
                "记录到异常事件队列",
                "定期复查（24 小时内）",
                "用户教育提醒"
            ])
        else:
            actions.append("记录到审计日志，无需即时行动")

        # 特定异常的针对性动作
        for anomaly in anomalies:
            if anomaly["type"] == "impossible_travel":
                actions.append("验证 VPN/代理使用情况")
            elif anomaly["type"] == "mass_data_exfiltration":
                actions.append("检查 DLP 告警，确认数据去向")

        return actions

    def identify_work_hours(self, timestamps):
        """
        识别用户主要工作时间
        """
        hours = [ts.hour for ts in timestamps]
        # 统计每小时出现频率
        hour_counts = pd.Series(hours).value_counts()
        peak_hours = hour_counts.nlargest(8).index.tolist()  # 取最活跃的 8 小时

        return {
            "start": min(peak_hours),
            "end": max(peak_hours)
        }

    def assign_peer_group(self, user_id, features):
        """
        将用户分配到同侪组（简化版：基于部门/职能）
        """
        user_info = self.get_user_info(user_id)
        return f"{user_info['department']}_{user_info['role']}"

    def geolocate_ip(self, ip_address):
        """
        IP 地理定位（示例，实际应使用 MaxMind GeoIP 等）
        """
        # 简化实现
        if ip_address.startswith("10.") or ip_address.startswith("192.168."):
            return {"country": "CN", "city": "Beijing", "is_corporate_network": True}
        else:
            # 实际应调用 GeoIP API
            return {"country": "UNKNOWN", "city": "UNKNOWN", "is_corporate_network": False}

# 使用示例
ueba = UEBAEngine()

# 为用户建立基线
baseline = ueba.build_user_baseline(user_id="john.doe", historical_days=90)
print(f"用户基线建立完成：平均每日 {baseline['avg_daily_queries']:.1f} 次查询")

# 检测当前活动
current_activity = {
    "user_id": "john.doe",
    "timestamp": datetime.now(),
    "query_count": 350,  # 当日查询数（异常高）
    "databases": ["production_db", "analytics_db", "external_partner_db"],  # 包含新数据库
    "tables": ["customers", "transactions", "employee_salaries"],
    "source_ip": "103.24.56.78",  # 境外 IP
    "total_downloaded_rows": 50000
}

result = ueba.detect_anomalies("john.doe", current_activity)

print(f"\n{'='*60}")
print(f"UEBA 异常检测结果")
print(f"{'='*60}")
print(f"用户：{result['user_id']}")
print(f"风险评分：{result['risk_score']}/100 ({result['risk_level']})")
print(f"\n检测到 {len(result['anomalies'])} 项异常：")
for i, anomaly in enumerate(result['anomalies'], 1):
    print(f"  {i}. [{anomaly['severity'].upper()}] {anomaly['type']}")

print(f"\n推荐响应动作：")
for action in result['recommended_actions']:
    print(f"  - {action}")
```

---

## 8.7.3 数据外泄检测

数据外泄检测需要综合 DAM、DLP、UEBA、网络流量监控等多数据源，构建完整的证据链。单一数据源可能产生误报（如用户正常导出报表），但多维度证据聚合可提高检测置信度。

### 多层检测机制

```python
class DataExfiltrationDetector:
    def __init__(self, dam_client, dlp_client, ueba_client, network_monitor):
        self.dam = dam_client
        self.dlp = dlp_client
        self.ueba = ueba_client
        self.network = network_monitor

    def detect_exfiltration(self, user_id, time_window_hours=24):
        """
        综合多数据源检测数据外泄
        """
        from datetime import timedelta
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=time_window_hours)

        # 收集多维度证据
        indicators = {
            "database_activity": self.analyze_database_activity(user_id, start_time, end_time),
            "file_operations": self.analyze_file_operations(user_id, start_time, end_time),
            "network_traffic": self.analyze_network_traffic(user_id, start_time, end_time),
            "dlp_incidents": self.get_dlp_incidents(user_id, start_time, end_time),
            "behavioral_anomalies": self.ueba.detect_anomalies(user_id, {"timestamp": end_time})
        }

        # 计算综合风险评分
        exfiltration_score = self.calculate_exfiltration_score(indicators)

        # 生成详细报告
        report = {
            "user_id": user_id,
            "analysis_period": f"{start_time} to {end_time}",
            "exfiltration_likelihood": exfiltration_score,
            "risk_level": "CRITICAL" if exfiltration_score > 0.7 else "HIGH" if exfiltration_score > 0.5 else "MEDIUM",
            "indicators": indicators,
            "evidence_timeline": self.build_evidence_timeline(indicators),
            "recommended_actions": self.generate_response_plan(exfiltration_score)
        }

        return report

    def analyze_database_activity(self, user_id, start_time, end_time):
        """
        分析数据库访问模式
        """
        queries = self.dam.get_user_queries(user_id, start_time, end_time)

        return {
            "total_queries": len(queries),
            "total_rows_accessed": sum(q["result_rows"] for q in queries),
            "unique_tables": len(set(q["table"] for q in queries)),
            "has_full_table_scan": any(not q["has_where_clause"] for q in queries),
            "avg_result_size": np.mean([q["result_rows"] for q in queries]),
            "suspicious_queries": [
                q for q in queries
                if q["result_rows"] > 10000 or not q["has_where_clause"]
            ]
        }

    def analyze_file_operations(self, user_id, start_time, end_time):
        """
        分析文件下载/复制行为
        """
        file_events = self.get_file_events(user_id, start_time, end_time)

        downloads = [e for e in file_events if e["action"] == "download"]
        usb_copies = [e for e in file_events if e["destination"] == "usb"]

        return {
            "total_downloads": len(downloads),
            "total_downloaded_size_gb": sum(e["size_bytes"] for e in downloads) / (1024**3),
            "usb_copy_events": len(usb_copies),
            "cloud_uploads": len([e for e in file_events if "cloud" in e.get("destination", "")]),
            "sensitive_files": [
                e for e in downloads
                if e.get("classification") in ["CONFIDENTIAL", "RESTRICTED"]
            ]
        }

    def analyze_network_traffic(self, user_id, start_time, end_time):
        """
        分析网络流量模式
        """
        traffic = self.network.get_user_traffic(user_id, start_time, end_time)

        return {
            "total_egress_gb": sum(t["bytes_out"] for t in traffic) / (1024**3),
            "external_destinations": len(set(t["dst_ip"] for t in traffic if not self.is_internal_ip(t["dst_ip"]))),
            "encrypted_traffic_ratio": len([t for t in traffic if t["is_encrypted"]]) / max(len(traffic), 1),
            "suspicious_destinations": [
                t["dst_ip"] for t in traffic
                if self.is_suspicious_destination(t["dst_ip"])
            ],
            "unusual_protocols": [
                t["protocol"] for t in traffic
                if t["protocol"] not in ["HTTPS", "HTTP", "SSH"]
            ]
        }

    def calculate_exfiltration_score(self, indicators):
        """
        综合评分算法
        """
        score = 0.0

        # 数据库活动（权重：0.3）
        db = indicators["database_activity"]
        if db["total_rows_accessed"] > 50000:
            score += 0.15
        if db["has_full_table_scan"]:
            score += 0.10
        if len(db["suspicious_queries"]) > 5:
            score += 0.05

        # 文件操作（权重：0.3）
        files = indicators["file_operations"]
        if files["total_downloaded_size_gb"] > 10:
            score += 0.15
        if files["usb_copy_events"] > 0:
            score += 0.10
        if len(files["sensitive_files"]) > 20:
            score += 0.05

        # 网络流量（权重：0.2）
        network = indicators["network_traffic"]
        if network["total_egress_gb"] > 5:
            score += 0.10
        if network["suspicious_destinations"]:
            score += 0.10

        # DLP 事件（权重：0.1）
        if len(indicators["dlp_incidents"]) > 3:
            score += 0.10

        # UEBA 异常（权重：0.1）
        if indicators["behavioral_anomalies"]["risk_score"] > 70:
            score += 0.10

        return min(score, 1.0)

    def build_evidence_timeline(self, indicators):
        """
        构建证据时间线
        """
        timeline = []

        # 添加数据库事件
        for query in indicators["database_activity"]["suspicious_queries"]:
            timeline.append({
                "timestamp": query["timestamp"],
                "type": "Database Query",
                "description": f"查询 {query['table']} 表，返回 {query['result_rows']} 行",
                "severity": "high" if query["result_rows"] > 10000 else "medium"
            })

        # 添加文件事件
        for file_event in indicators["file_operations"]["sensitive_files"]:
            timeline.append({
                "timestamp": file_event["timestamp"],
                "type": "File Download",
                "description": f"下载敏感文件：{file_event['filename']} ({file_event['size_bytes']/1024/1024:.1f}MB)",
                "severity": "high"
            })

        # 添加 DLP 事件
        for incident in indicators["dlp_incidents"]:
            timeline.append({
                "timestamp": incident["timestamp"],
                "type": "DLP Violation",
                "description": f"违反策略：{incident['policy_name']}",
                "severity": "critical"
            })

        # 按时间排序
        timeline.sort(key=lambda x: x["timestamp"])

        return timeline

    def generate_response_plan(self, score):
        """
        生成响应计划
        """
        if score > 0.7:
            return [
                "立即冻结用户账号",
                "隔离用户终端设备",
                "启动事件响应流程 (IR-EXFIL)",
                "通知: CISO、法务、HR",
                "开始取证调查",
                "审查用户最近 30 天所有活动",
                "联系外部网络安全取证团队（如需要）"
            ]
        elif score > 0.5:
            return [
                "限制用户数据访问权限",
                "增强监控（记录屏幕/键盘）",
                "通知用户直属经理进行面谈",
                "人工审查所有可疑活动",
                "保留取证数据 48 小时"
            ]
        else:
            return [
                "记录到安全事件队列",
                "持续监控 72 小时",
                "如有进一步可疑活动，升级处理"
            ]

# 使用示例
detector = DataExfiltrationDetector(dam, dlp, ueba, network_monitor)

# 检测潜在数据外泄
report = detector.detect_exfiltration(user_id="john.doe", time_window_hours=24)

print(f"\n{'='*70}")
print(f"数据外泄检测报告")
print(f"{'='*70}")
print(f"用户：{report['user_id']}")
print(f"分析时段：{report['analysis_period']}")
print(f"外泄可能性：{report['exfiltration_likelihood']:.1%} ({report['risk_level']})")

print(f"\n关键指标：")
print(f"  - 数据库访问: {report['indicators']['database_activity']['total_rows_accessed']:,} 行")
print(f"  - 文件下载: {report['indicators']['file_operations']['total_downloaded_size_gb']:.2f} GB")
print(f"  - 网络外传: {report['indicators']['network_traffic']['total_egress_gb']:.2f} GB")
print(f"  - DLP 事件: {len(report['indicators']['dlp_incidents'])} 起")

print(f"\n证据时间线：")
for event in report['evidence_timeline'][:5]:  # 显示前 5 个事件
    print(f"  [{event['timestamp']}] {event['type']}: {event['description']}")

print(f"\n推荐响应动作：")
for action in report['recommended_actions']:
    print(f"  {action}")
```

---

## 8.7.4 审计日志管理

审计日志是合规证据的基础，也是事件调查的唯一真实来源。审计日志管理需要满足两个核心要求：完整性（不可篡改）与可用性（快速检索）。

### 不可变审计存储

借鉴区块链思想，通过哈希链保证审计日志不可篡改。每条审计记录包含前一条记录的哈希，任何篡改会导致链断裂。

```python
import hashlib
import hmac
from cryptography.fernet import Fernet

class ImmutableAuditLog:
    """
    实现 WORM (Write Once Read Many) 特性的审计日志系统
    """
    def __init__(self, storage_backend, encryption_key):
        self.storage = storage_backend
        self.cipher = Fernet(encryption_key)
        self.chain_state = self.load_chain_state()  # 区块链式链接

    def write_audit_record(self, record):
        """
        写入审计记录（不可篡改）
        """
        # 1. 生成唯一 ID
        record_id = self.generate_record_id()

        # 2. 添加完整性元数据
        record_with_metadata = {
            "record_id": record_id,
            "timestamp": datetime.now().isoformat(),
            "data": record,
            "previous_hash": self.chain_state["last_hash"],  # 链接到前一条记录
            "sequence_number": self.chain_state["sequence"] + 1
        }

        # 3. 计算哈希（包含前一条记录的哈希，形成链）
        record_hash = self.calculate_hash(record_with_metadata)
        record_with_metadata["record_hash"] = record_hash

        # 4. 数字签名（可选，用于法律证据）
        # record_with_metadata["signature"] = self.sign_record(record_hash)

        # 5. 加密敏感字段
        encrypted_record = self.encrypt_sensitive_fields(record_with_metadata)

        # 6. 写入存储（WORM 模式）
        self.storage.write(record_id, encrypted_record, immutable=True)

        # 7. 更新链状态
        self.chain_state = {
            "last_hash": record_hash,
            "sequence": record_with_metadata["sequence_number"],
            "last_record_id": record_id
        }
        self.save_chain_state()

        return record_id

    def calculate_hash(self, record):
        """
        计算记录哈希
        """
        import json
        # 排除 hash 字段本身
        record_copy = {k: v for k, v in record.items() if k != "record_hash"}
        canonical_json = json.dumps(record_copy, sort_keys=True)
        return hashlib.sha256(canonical_json.encode()).hexdigest()

    def verify_chain_integrity(self, start_record_id=None, end_record_id=None):
        """
        验证审计链完整性
        """
        records = self.storage.read_range(start_record_id, end_record_id)

        for i, record in enumerate(records):
            # 验证 1：哈希正确性
            calculated_hash = self.calculate_hash(record)
            if calculated_hash != record["record_hash"]:
                return {
                    "valid": False,
                    "error": f"Hash mismatch at record {record['record_id']}",
                    "tampered_record": record["record_id"]
                }

            # 验证 2：链连续性
            if i > 0:
                if record["previous_hash"] != records[i-1]["record_hash"]:
                    return {
                        "valid": False,
                        "error": f"Chain break detected at record {record['record_id']}",
                        "tampered_record": record["record_id"]
                    }

            # 验证 3：序列号连续性
            if i > 0:
                if record["sequence_number"] != records[i-1]["sequence_number"] + 1:
                    return {
                        "valid": False,
                        "error": f"Sequence number gap at record {record['record_id']}"
                    }

        return {
            "valid": True,
            "verified_records": len(records),
            "start_sequence": records[0]["sequence_number"],
            "end_sequence": records[-1]["sequence_number"]
        }

    def encrypt_sensitive_fields(self, record):
        """
        加密审计记录中的敏感字段
        """
        sensitive_fields = ["user", "source_ip", "query", "file_path"]

        encrypted_record = record.copy()
        for field in sensitive_fields:
            if field in record.get("data", {}):
                original_value = record["data"][field]
                encrypted_value = self.cipher.encrypt(original_value.encode()).decode()
                encrypted_record["data"][field] = encrypted_value

        return encrypted_record

    def search_audit_logs(self, query_params):
        """
        审计日志检索（支持多条件）
        """
        results = []

        # 示例查询参数
        user = query_params.get("user")
        start_time = query_params.get("start_time")
        end_time = query_params.get("end_time")
        event_type = query_params.get("event_type")
        risk_level = query_params.get("risk_level")

        # 从存储读取记录（实际应使用索引）
        all_records = self.storage.scan(start_time, end_time)

        for record in all_records:
            # 解密敏感字段用于匹配
            decrypted_record = self.decrypt_record(record)

            # 应用过滤器
            if user and decrypted_record["data"].get("user") != user:
                continue
            if event_type and decrypted_record["data"].get("event_type") != event_type:
                continue
            if risk_level and decrypted_record["data"].get("risk_level") != risk_level:
                continue

            results.append(decrypted_record)

        return results

    def generate_compliance_report(self, framework="SOX", period="2024-Q1"):
        """
        生成合规审计报告
        """
        if framework == "SOX":
            return self.generate_sox_report(period)
        elif framework == "GDPR":
            return self.generate_gdpr_report(period)
        elif framework == "PCI-DSS":
            return self.generate_pci_report(period)

    def generate_sox_report(self, period):
        """
        生成 SOX 审计报告（示例）
        """
        # SOX 要求：所有对财务数据的访问都有审计记录
        financial_db_access = self.search_audit_logs({
            "database": "financial_db",
            "start_time": self.get_period_start(period),
            "end_time": self.get_period_end(period)
        })

        report = {
            "framework": "SOX Section 404",
            "period": period,
            "generated_at": datetime.now().isoformat(),

            "audit_completeness": {
                "total_access_events": len(financial_db_access),
                "events_with_full_audit_trail": len([e for e in financial_db_access if self.has_complete_trail(e)]),
                "compliance_rate": "100%"  # 计算
            },

            "access_control_review": {
                "privileged_access_events": len([e for e in financial_db_access if e["data"]["is_privileged"]]),
                "all_privileged_access_reviewed": True,
                "unauthorized_access_detected": 0
            },

            "segregation_of_duties": {
                "violations_detected": 0,  # 示例
                "details": "No user has both data entry and approval rights"
            },

            "audit_log_integrity": self.verify_chain_integrity(),

            "certification": "本报告基于完整且未篡改的审计日志生成，符合 SOX 404 要求。"
        }

        return report

# 使用示例
audit_log = ImmutableAuditLog(storage_backend=S3WORMStorage(), encryption_key=Fernet.generate_key())

# 写入审计记录
record_id = audit_log.write_audit_record({
    "event_type": "database_query",
    "user": "john.doe",
    "database": "financial_db",
    "table": "transactions",
    "action": "SELECT",
    "result_rows": 1500,
    "risk_level": "medium"
})

print(f"审计记录已写入: {record_id}")

# 验证审计链完整性
integrity_check = audit_log.verify_chain_integrity()
print(f"\n审计链完整性验证: {'通过' if integrity_check['valid'] else '失败'}")

# 生成合规报告
sox_report = audit_log.generate_compliance_report(framework="SOX", period="2024-Q1")
print(f"\nSOX 合规报告已生成:")
print(f"  - 总访问事件: {sox_report['audit_completeness']['total_access_events']}")
print(f"  - 合规率: {sox_report['audit_completeness']['compliance_rate']}")
```

---

## 8.7.5 监控效果度量

监控体系的有效性需要通过量化指标持续评估。以下 KPI 体系覆盖检测能力、响应效率、合规达标、系统健康四个维度。

```python
# 数据安全监控 KPI（内部参考口径，非行业统一标准）
monitoring_kpis = {
    "detection_capability": {
        "threats_detected": 127,  # 季度检测威胁数
        "false_positive_rate": 0.12,  # 12% 误报率
        "mean_time_to_detect_minutes": 8,  # 平均检测时间
        "detection_coverage": {
            "database_access": 1.0,  # 100% 覆盖
            "file_operations": 0.95,
            "network_traffic": 0.85,
            "cloud_api_calls": 0.90
        }
    },

    "response_efficiency": {
        "mean_time_to_respond_minutes": 22,
        "incidents_auto_resolved": 45,
        "incidents_requiring_manual_intervention": 12,
        "automation_rate": 0.79  # 79% 事件自动化处理
    },

    "audit_compliance": {
        "audit_log_completeness": 0.998,  # 99.8% 完整性
        "audit_log_retention_days": 2555,  # 7 年
        "chain_integrity_verifications": 1200,  # 季度验证次数
        "integrity_failures": 0,
        "compliance_report_generation_time_hours": 2
    },

    "operational_health": {
        "monitoring_system_uptime": 0.9998,
        "log_ingestion_rate_events_per_second": 15000,
        "storage_utilization": 0.68,
        "query_performance_p95_ms": 350
    }
}

def generate_monitoring_dashboard():
    """
    生成监控效果仪表板
    """
    return f"""
╔══════════════════════════════════════════════════════════════╗
║ 数据安全监控效果仪表板 - Q1 2024 ║
╠══════════════════════════════════════════════════════════════╣
║ ║
║ 检测能力 ║
║   威胁检测数量: {monitoring_kpis['detection_capability']['threats_detected']} 起 ║
║   误报率: {monitoring_kpis['detection_capability']['false_positive_rate']:.1%}（目标: <15%）║
║   平均检测时间: {monitoring_kpis['detection_capability']['mean_time_to_detect_minutes']} 分钟（目标: <10 分钟）║
║ ║
║ 响应效率 ║
║   平均响应时间: {monitoring_kpis['response_efficiency']['mean_time_to_respond_minutes']} 分钟（目标: <30 分钟）║
║   自动化处理率: {monitoring_kpis['response_efficiency']['automation_rate']:.1%}（目标: >70%）║
║ ║
║ 审计合规 ║
║   日志完整性: {monitoring_kpis['audit_compliance']['audit_log_completeness']:.2%} ║
║   完整性验证失败: {monitoring_kpis['audit_compliance']['integrity_failures']} 次 ║
║   保留期限: {monitoring_kpis['audit_compliance']['audit_log_retention_days']//365} 年（符合 SOX 要求）║
║ ║
║ 系统健康 ║
║   系统可用性: {monitoring_kpis['operational_health']['monitoring_system_uptime']:.2%} ║
║   日志接收速率: {monitoring_kpis['operational_health']['log_ingestion_rate_events_per_second']:,} 事件/秒 ║
║ ║
╚══════════════════════════════════════════════════════════════╝
"""

print(generate_monitoring_dashboard())
```

**KPI 使用说明**：

- 威胁检测数量：统计触发 HIGH/CRITICAL 告警的独立事件（去重后）
- 误报率计算：误报数 / (误报数 + 真阳性数)，需人工复审确认
- 平均检测时间：从异常行为发生到系统生成告警的时长，通过事后分析计算
- 自动化处理率：通过 SOAR 自动遏制或修复的事件占比（无需人工介入）
- 日志完整性：通过哈希链完整性验证，任何失败立即触发 CRITICAL 告警

---

## 导航

**[← 上一节：8.6 数据丢失防护（DLP）](./8.6_data_loss_prevention.md)** | **[返回章节目录](./README.md)** | **[下一节：8.8 跨境数据治理 →](./8.8_cross_border_data_governance.md)**

---

**© 2025 AI-ESA Project. Licensed under CC BY-NC-SA 4.0**
